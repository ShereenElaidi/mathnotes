\documentclass[11pt]{scrartcl}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{amsfonts}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{xcolor} 
\usepackage{enumitem}
\newcommand{\R}[0]{\mathbb{R}}
\addtokomafont{section}{\rmfamily\centering\scshape}
% math environments 
\usepackage[utf8]{inputenc}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{ex}{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% definition
\newcommand{\dfn}[1]{\textbf{\underline{#1}}}
\newcommand{\dist}[0]{\mathcal{F}}
\newcommand{\pr}[1]{\mathbb{P}[#1]} 
\newcommand{\stat}[0]{T(X_1, ..., X_n )} 

% converge in probability 
\newcommand{\cvp}[0]{\overset{p}{\to}}

% sample mean
\newcommand{\smean}[0]{\frac{1}{n} \sum_{i=1}^n x_i} 

% sample variance
\newcommand{\svar}[0]{\frac{1}{(n-1)} \sum_{i=1}^n (x_i - \overline{x})^2}

% expected value 
\newcommand{\EX}[1]{\mathbb{E}\left[#1 \right]}  
\newcommand{\EXth}[1]{\mathbb{E}_\theta \left[ #1 \right]}

% integral
\newcommand{\idx}[2]{\int_{#1}^{#2}}

% vector
\newcommand{\vect}[1]{\mathbf{#1}}


\title{\textbf{Math 458: Differential Geometry}}
\author{Shereen Elaidi}
\date{Winter 2020 Term}


\begin{document}

\maketitle
\tableofcontents
\pagestyle{fancy}
\lhead{Math 458: Differential Geometry}
\chead{Winter 2020 -- Summary}
\rhead{Page \thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\setlength{\tabcolsep}{0.5em} % for the horizontal padding
{\renewcommand{\arraystretch}{1.2}% for the vertical padding

\section{Introduction}
This course is about differential geometry of curves, surfaces, and manifolds in $\R^3$ + integration with differential forms. 
\subsection{Dual Spaces}
I am including this since I did not learn about dual spaces in my linear algebra class.

\begin{definition}[Linear Functional]
	Let $V$ be a vector space over $K$. A map $\phi: V \rightarrow K$ is a \dfn{linear functional} if $\forall$ $v , u \in V$, $a, b \in K$: 
	\begin{align}
		\phi(au+ bv) = a \phi(u) + b \phi(v) 	
	\end{align}
\end{definition}
Examples of linear functionals: 
\begin{enumerate}[noitemsep]
	\item Let $V$ be the vector space of polynomials in $t$ over $\R$. Define the definite integral operator $J(p(t)):= \idx{0}{1} p(t)dt$. By the linearity of integration, this is a linear functional on $V$. 
	\item Let $V$ be the vector space of $n \times n$ matrices with real coefficients. Then, define the trace map: $T: V \rightarrow \R$ as the trace of a matrix $A$. This is a linear functional on $V$. 
\end{enumerate}

\begin{definition}[Dual Space]
	Let $V$ be a vector space over a field $K$. Then, the set of all linear functionals on $V$ over $K$ is a vector space over $K$ with addition and scalar multiplication defined by: 
	\begin{align*}
		& (\phi + \sigma)(v) := \phi(v) + \sigma(v)  \\
		& (k \phi)(v) = k \phi(v) 
	\end{align*}
	This vector space is called the \dfn{dual space} of $V$, denoted by $V^*$. 
\end{definition}

\begin{ex} 
Consider $V = K^n$. This is the vector space of all $n$-tuples, written as column vectors. Then, $V^*$ can be thought of as the space of all row vectors. We can represent any linear functional $\phi = (a_1, ..., a_n) \in V^*$ as a \dfn{linear form}: 
\begin{align*}
	\phi(x_1, ..., x_n) = \begin{bmatrix}
 	a_1 & a_2 & \cdots & a_n 	
 \end{bmatrix} \begin{bmatrix}
 	x_1 & x_2  & \cdots & x_n
 \end{bmatrix}^t = a_1 x_1 + ... + a_n x_n
\end{align*}	
\end{ex}

When you choose a basis for a vector space $V$, you obtain an induced basis on the dual $V^*$: 
\begin{theorem}
	Suppose $\{ v_1, ..., v_n \}$ is a basis of $V$ over $K$. Let $\phi_1 , .., \phi_n \in V^*$ be linear functionals defined by: 
	\begin{align}
		\phi_i(v_j) := \delta_{ij}	
	\end{align}
	Then, $\{ \phi_1, ..., \phi_n \}$ is a basis of $V^*$. This basis is called the \dfn{dual basis}. 
\end{theorem}

Theorems giving the relationships between bases and their dual bases: 
\begin{theorem}
	Let $\{ v_1, ..., v_n \}$ be a basis of $V$; let $\{ \phi_1 , ..., \phi_n \}$ be the dual basis in $V^*$. Then: 
	\begin{enumerate}[noitemsep]
		\item $\forall u \in V$, $u  = \phi_1(u)v_1 + ... + \phi_n(u) v_n$ 
		\item For any linear functional $\sigma \in V^*$, $\sigma = \sigma(v_1) \phi_1 + ... + \sigma(v_n) \phi_n$. 
	\end{enumerate}
\end{theorem}
	The change of basis on a vector space induces a change of basis on its dual. This is the point of the following theorem: 
\begin{theorem}
	Let $\{ v_1, ..., v_n \}$ and $\{ w_1, ..., w_n \}$ be bases of $V$ and let $\{ \phi_1, ... , \phi_n \}$ and $\{ \sigma_1, ..., \sigma_n \}$ be bases of $V^*$, dual to $\{ v_i \}$ and $\{ w_i \}$, respectively. If $P$ is the change of basis matrix from $\{ v_i \}$ to $\{w_i \}$, then $(P^{-1})^t$ is the change of basis matrix from $\{ \phi_i \}$ to $\{\ \sigma_i \}$. 
\end{theorem}

\begin{theorem}
	If $V$ is a \emph{finite-dimensional} vector space, then $V \cong V^{**}$. 
\end{theorem}

The following definition-theorem would have been very useful for the first homework :-) 

\begin{definition}[Transpose of a Linear Mapping] 
	Let $U$, $V$ be vector spaces over $K$. Let $T: V \rightarrow U$ be an arbitrary linear mapping. Let $\phi \in U^*$ be a linear functional. Since linearity is stable under compositions, the composition map $\phi \circ T$ is a linear map $V \rightarrow K$, and this $(\phi \circ T) \in V^*$. Define the following map from $U^* \rightarrow V^*$: 
	\begin{align*}
		\phi \mapsto \phi \circ T 
	\end{align*}
	This map as defined is called the \dfn{transpose of T}. Formally: for each $v \in V$, the transpose map gives us: 
	\begin{align}
		(T^t(\phi))(v) = \phi(T(v)) 	
	\end{align}
\end{definition}


\begin{theorem}
	The transpose mapping $T^t$ is linear. 
\end{theorem}
\begin{theorem}
	Let $T: V \rightarrow U$ be linear. Let $A$ be the matrix representation of $T$ with respect to the bases $\{v_i \}$ of $V$ and $\{ u_i \}$ of $U$. Then, the transpose matrix $A^t$ is the matrix representation of $T^t : U^* \rightarrow V^*$ relative to the bases dual to $\{ u_i \}$ and $\{ v_i \}$. 
\end{theorem}


\subsection{Notions from Multivariable Cal}
\begin{definition}[Differential]
	The \dfn{differential} of a map $f: \R^m \rightarrow \R^n$ at the point $\phi \in \R^m$ is the best linear approximation of the map at the point $\phi$: 
	\begin{align}
		f(q) = f(p) + Df(p) \cdot (q-p) + O(||q-p||) 	
	\end{align}
	Here, $Df(p)$ is the differential, which is an $n \times m$ matrix. 
\end{definition}

\begin{theorem}[Inverse Function Theorem]
	Let $f: \R^n \rightarrow \R^n$ be continuously differentiable in an open set containing $a$ and det$f'(a) \neq 0$. Then, there is an open set $V$ containing $a$ and an open set $W$  containing $f(a)$ such that $f: V \rightarrow W$ has a continuous inverse $f{-1}: W \rightarrow V$ which is differentiable and $\forall y \in W$ satisfies: 
	\begin{align}
		(f^{-1})'(y) = [ f'(f^{-1}(y))]^{-1} 	
	\end{align}
\end{theorem}

\begin{theorem}[Implicit Function Theorem]
	Let $f: \R^n \times \R^m \rightarrow \R^m$ be a continuously differentiable function in an open set containing $(a,b)$ and $f(a,b) =0$. Let $M$ be the $m \times m$ matrix: 
	\begin{align*}
		(D_{n+j}f^i (a,b)) 
	\end{align*}
	with $1 \leq i, i \leq m$. If det$(M) \neq 0 $, then there exists an open set $A \subseteq \R^n$ containing $a$ and an open set $B \subseteq \R^m$ containing $b$ with the following property: $\forall$ $ x \in A$, $\exists_1$ $g(x) \in B$ such that $f(g, g(x) ) =0$. Moreover, the function $g$ is differentiable. 
\end{theorem} 	


\begin{definition}[Line Integral]
	Let $\Omega \subseteq \R^n$ be open. Let $F$ be a smooth vector field. Let $\gamma: [a,b] \rightarrow \Omega$ be an oriented curve. Then, the \dfn{line integral} of $F$ over $\gamma$ is defined as: 
	\begin{align*}
		\idx{\gamma}{} F \cdot d\gamma := \idx{a}{b} F(\gamma(t)) \cdot g'(t) dt 
	\end{align*}
\end{definition}

\begin{definition}[Two-Dimensional Curl]
	Let $F$ be a smooth vector field. Then, the two-dimensional \dfn{curl} is defined as: 
	\begin{align*}
		\text{curl} (F) := \partial_x F_y - \partial_y F_x 
	\end{align*}
\end{definition}


\begin{definition}[Unit Normal Vector of a Parameterised Surface] 
	Let $\mathbb{X}: K \subseteq \R^2 \rightarrow \R^3$ be a parameterisation. Then, the \dfn{unit normal vectors} are: 
	\begin{align*}
		n := \pm \frac{\partial_u \mathbb{X} \times \partial_v \mathbb{X}}{|| \partial_u \mathbb{X} \times \partial_v \mathbb{X} ||}
	\end{align*}
\end{definition}

We will state some basic (and important) results from vector calculus: the divergence theorem, green's theorem, and stokes' theorem. 
\subsubsection{Divergence}
\begin{theorem}[Divergence Theorem]
Let $F$ be a smooth vector field and let $\Omega$ be a bounded domain with outer normal $n$. Then: 
\begin{align}
	\iiint_{\Omega} \text{div}F d \Omega = \iint_{\partial \Omega} F \cdot n dS	
\end{align}
Where the \dfn{divergence} of a smooth vector field $F$ is given by: 
\begin{align*}
	\text{div} F := \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z}
\end{align*}
\end{theorem}
We can write the divergence of a vector field as a dot product with the del operator: 
\begin{align*}
	\text{div} F = \nabla \cdot F 
\end{align*}

\subsubsection{Green's Theorem}
From the divergence theorem, we can deduce Green's theorem. It is given by: 
\begin{theorem}[Green's Theorem]
	Let $P(x,y)$ and $Q(x,y)$ be smooth functions $\R^2 \rightarrow \R$. Let $\Omega \subseteq \R^2$ be bounded. Then: 
	\begin{align}
		\iint_\Omega \left[ 	\frac{\partial Q(x,y)}{\partial x} - \frac{\partial P(x,y)}{\partial y}	\right] dx dy = \idx{\mathcal{C}}{} P(x,y)dx + Q(x,y)dy	
	\end{align}
	where $\mathcal{C} = \partial \Omega$. 
\end{theorem}

There is also a formulation for Green's theorem in terms of the curl of a vector field. 

\begin{theorem}[Green's Theorem II]
	Let $K$ be a region bounded by a closed, oriented curve $\gamma$. Then, for a smooth vector field $F$ in $K$, we have: 
	\begin{align}
		\idx{\gamma}{} F \cdot d \gamma = \idx{K}{} \text{curl}(F) 	
	\end{align}
\end{theorem}

Finally, we have Stokes' Theorem. 

\begin{theorem}
	Let $\Omega$ be a smooth, oriented surface bounded by a closed, smooth boundary curve $\partial \Omega$ which is positively oriented. Let $F$ be a smooth vector field. Then: 
	\begin{align}
		\idx{\partial \Omega}{} F \cdot dr = \iint_{\Omega} \text{curl} F \cdot dS 	
	\end{align}

\end{theorem}

\section{Manifolds in $\R^3$}
The aim of this part of the course is to build up to integration on manifolds and the invariant Stokes' theorem. The main purpose of this sections is to develop \emph{coordinate-free} calculus, which clarifies the essence of what is happening (sometimes coordinates can be noisy). 
\subsection{Definitions}

\begin{definition}[K-Dimensional Manifold]
	A subset $M \subseteq \R^n$ is called a \dfn{k-dimensional manifold} in $\R^n$ if $\forall x \in M$, the following condition is satisfied: $\exists$ an open set $U$ containing $x$ and open set $V \subseteq \R^n$, and a diffeomorphism $h: U \rightarrow V$ such that 
	\begin{align*}
		h (U \cap M) & = V \cap (\R^k \times \{ 0 \} ) \\
					& = \{ y \in V\ |\ y^{k+1} = ... = y^n = 0 \} 
	\end{align*}
	In other words, we require that $U \cap M$ is, up to diffeomorphism, $\R^k \times \{ 0 \}$. 
\end{definition}

\begin{definition}[$C^\infty$-function] There are two definitions. 
	\begin{enumerate}[noitemsep] 
		\item $f: M \rightarrow \R$ is $C^\infty$ if it is $C^\infty$ in each parameterisation. 
		\item $f: M \rightarrow \R$ is $C^\infty$ if it is locally the restriction of a smooth function of the ambient space: $\forall p \in M$, $\exists V \subseteq \R^n$, $V$ open, $ p \in V$, and $F: V \rightarrow \R$ with $F|_{M \cap V} = f$. 
	\end{enumerate}
\end{definition}

Before we can do calculus, we need to define vector fields in a \emph{coordinate-free} way on a manifold $M$. 

\begin{definition}[Vector Field $V$ on $M$]
	The \dfn{vector field} $V$ on $M$ is defined as a function $C^\infty(M) \rightarrow C^\infty(M)$ satisfying three properties: 
	\begin{enumerate}[noitemsep]
		\item $v(f+g) = v(f) + v(g) $ (Linearity I) 
		\item $v (\alpha f) = \alpha v(f) $ (Linearity II) 
		\item $v(fg) = fv(g) + gv(f)$ (Leibniz Law; captures the essence of differentiation)
	\end{enumerate}
\end{definition}

Using this, we can define a \dfn{derivation} at $x \in \R^n$. First take a derivation $v \in \R^n$, and set: 
\begin{align}
	v(f) := \frac{d}{dt} \left[ f(x+tv) \right]_{t =v} 	
\end{align}
This is a \dfn{directional derivative} in the direction $v$. 

\begin{definition}[Tangent Bundle]
	Given a manifold $M^n$, you can package together all the tangent spaces together into a $2n$-dimensional manifold. You'd then obtain a vector bundle called the \dfn{tangent bundle}: 
	\begin{align*}
		T(M) := \bigsqcup_{p \in M} T_p(M) 
	\end{align*}
\end{definition}

\subsection{Smooth Maps from $M^m \rightarrow N^n$}
Let $M^m$ and $N^n$ be two manifolds. Consider a smooth map $g$ between them. Fix a point $p \in M^m$. The map $g$ induces a map on the tangent spaces. This map, denoted: 
\begin{align*}
	 D_{g_p}(v): T_p(M) \rightarrow T_{g(p)}(N)
\end{align*}
	is called the \dfn{differential} or \dfn{push-forward}. Here, $v$ is a derivation at $p \in M$ and $f$ is a function on $N$. 


\begin{definition}[Cotangent Space]
	The \dfn{cotangent space} is denoted by $T^*_p(M)$. It is the dual space of $T_p(M)$. Functions on $M$ give elements of $T_p^*(M)$ in the following way: 
	\begin{align*}
		df(v) := v(f) 
	\end{align*}
	where $v \in T_p(M)$. $v(f)$ is a derivation of $f$ in the direction $v$. 
\end{definition}
\subsection{Change of Coordinates}

\subsection{Multi-Linear Algebra}
\begin{definition}[$k$-linear map]
	Let $V^k := V \times \cdots \times V$ ($k$ times). A function $f:V^k \rightarrow \R$ is called \dfn{k-linear} if it is linear in each of its $k$ arguments. 
\end{definition}
A $k$-linear function on $V$ is also called a \dfn{k-tensor} on $V$. 

\begin{definition}[Symmetric/Alternating]
	A $k$-linear function $f: V^k \rightarrow \R$ is \dfn{symmetric} if: 
	\begin{align*}
		f(v_{\sigma(1)}, ..., v_{\sigma(k)}) = f(v_1, ..., v_k)
	\end{align*}
	for all permutations $\sigma \in S_k$ (symmetric group on $k$ letters); it is said to be \dfn{alternating} if
	\begin{align*}
		f(v_{\sigma(1)}, ..., v_{\sigma(k)} ) = (\text{sgn}(\sigma)) f(v_1, ..., v_n) 
	\end{align*}
\end{definition}
Examples of symmetric functions: 
\begin{itemize}[noitemsep]
	\item the dot product, $f(v,w) := v \cdot w$ on $\R^n$. 
\end{itemize}
Examples of alternating functions: 
\begin{itemize}[noitemsep]
	\item $f(v_1, ..., v_n) := \det [v_1, ..., v_n]$
	\item Cross product $v \times w$ on $\R^3$. 
	\item Generalisation of a cross product: let $f, g: V \rightarrow \R$ on a vector space $V$. Define $f \wedge g: V \times V \rightarrow \R$ by: 
	\begin{align*}
		( f \wedge g ) (u,v) := f(u) g(v) - f(v) g(u) 
	\end{align*}
	(special case of the wedge product). 
\end{itemize}
The space of all alternating $k$-linear functions on a vector space $V$ is denoted by $A_k(V)$. When $k=0$, a $0$-covector is a constant $\Rightarrow$ $A_0(V)$ is the vector space $\R$. A 1-covector is a covector. 

\begin{definition}[Tensor Product]
	Let $f$ be a $k$-linear function and $g$ an $l$-linear function on a vector space $V$. The \dfn{tensor product} is a $(k+l)-$linear function $f \otimes g$ defined as: 
	\begin{align}
		(f \otimes g)(v_1, ..., v_{k+l}) := f(v_1, ..., v_k) g(v_{k+1}, ..., v_{k+1}) 	
	\end{align}

\end{definition}

In order to motivate the next definition, assume that we have two multilinear functions $f$, $g$ on a vector space $V$. We would like to have a product that is alternating. This is why we define the wedge product: 

\begin{definition}[Wedge Product]
	Let $f \in A_k(V)$ and $g \in A_l(V)$. Then, the \dfn{wedge product} or \dfn{exterior product} is defined as:
	\begin{align*}
		f \wedge g := \frac{1}{k!l!} A(f \otimes g) 
	\end{align*}
	This can be written out explicitly as: 
	\begin{align*}
		(f \wedge g)(v_1, ..., v_{(k+l)}) = \frac{1}{k!l!} \sum_{ \sigma \in S_{k+l}} f(v_{\sigma(1)}, ..., v_{\sigma(k)} ) g(v_{\sigma(k+1)}, ..., v_{\sigma(k+l)}) 
	\end{align*}
\end{definition}
Remarks: 
\begin{itemize}[noitemsep]
	\item When $k=0$, this corresponds to scalar multiplication. 
	\item The coefficient $1/l!k!$ compensates for repetitions in the sum. 
\end{itemize}

\begin{prop}
	The wedge product is anti-commutative: if $f \in A_k(V)$ and $g \in A_l (V)$, then: 
	\begin{align*}
		f \wedge g = (-1)^{kl} g \wedge f
	\end{align*} 
\end{prop}


\subsection{Differential Forms in $M^n$}
Differential $k$-forms assign $k$-covectors on the tangent space to each point of an open set $\Omega$. There is a notion of differentiation for differential forms -- the exterior derivative. This is something that turns out to be intrinsic to the manifold. 

\begin{definition}[Differential One Form]
	A \dfn{covector field} or \dfn{differential 1-form} on an open subset $\Omega \subseteq \R^n$ is a function $\omega$ that assigns to each point $p \in \Omega$ a covector $\omega_p \in T_p^*(\R^n)$. 
\end{definition}
Given a $C^\infty$ function $f: \Omega \rightarrow \R$, we can construct the one-form called the \dfn{differential of $f$}, denoted $df$ as follows: let $p \in \Omega$ and let $X_p \in T_p(\Omega)$. Then, define: 
\begin{align*}
	(df)_p (X_p) := X_p 
\end{align*}

\begin{prop}
	Let $x^1, ..., x^n$ be the standard coordinates on $\R^n$. Then, at each point $p \in \R^n$, $\{ (dx^1)_p, ..., (dx^n)_p \}$ is the basis of the cotangent space $T_p^*(\R^n)$ dual to the basis $\{ [\partial/\partial x^1]_p, ..., [\partial / \partial x^n]_p \}$ for the tangent space $T_p(\R^n)$. 
\end{prop}

\begin{prop}[Differential in terms of coordinates]
	If $f: \Omega \rightarrow \R^n$ is $C^\infty$ on $\Omega \subseteq \R^n$ open, then: 
	\begin{align*}
		df = \sum \frac{\partial f}{\partial x^i} dx^i
	\end{align*}
\end{prop}

\begin{definition}[Differential form of degree $k$]
	A \dfn{differential k-form} on $\Omega \subseteq \R^n$ is a function that assigns to each point $p \in \Omega$ an alternating $k$-linear function on the tangent space $T_p(\R^n)$; i.e., $\omega_p \in A_k(T_p(\R^n))$. 
\end{definition}
\begin{itemize}[noitemsep]
	\item Basis for $A_k(T_p (\R^n))$: 
	\begin{align*}
		dx_p^I = dx_p^{i_1} \wedge \cdots \wedge dx_p^{i_k},\ 1 \leq i_1 < \cdots < i_k \leq n
	\end{align*}
	\item For each point $p \in \Omega$, $\omega_p$ can be expressed as a linear combination: 
	\begin{align*}
		\omega_p = \sum a_I(p) dx_p^I,\ 1 \leq i_1 < \cdots < i_k \leq n
	\end{align*}
	\item General $k$-form on $\Omega$: 
	\begin{align*}
		\omega = \sum a_I dx^I 
	\end{align*}
	\item $\Omega^k(U)$ is the vector space of $C^\infty$ $k$-forms on $U$. 
	\begin{itemize}[noitemsep]
		\item $0$-form on $U$ is a smooth function on $U$. 
	\end{itemize}
\end{itemize}
The wedge product of two $k$-forms: 
\begin{align*}
	\omega \wedge \tau := \sum_{I,J \text{ disjoint}} (a_I b_J) dx^I \wedge dx^J 
\end{align*}
To make this concrete: let $x,y,z$ be the coordinates on $\R^3$. Then:
\begin{itemize}[noitemsep]
	\item $C^\infty$ 1-forms are: 
	\begin{align*}
		fdx + gdy + hdxz 
	\end{align*}
	where $h,y,h$ range over all smooth functions on $\R^3$.
	\item $C^\infty$ 2-forms are: 
	\begin{align*}
		f dy \wedge dz + g dx \wedge dz + h dx \wedge dy 
	\end{align*}
	\item $C^\infty$ 3-forms are: 
	\begin{align*}
		f dx \wedge dy \wedge dz 
	\end{align*}
\end{itemize}
Here are some worked examples of taking the wedge products between differential forms. 

\begin{ex}
	Consider the 2-form $dx \wedge dy$. Express this in polar coordinates. 
	\newline
	\textbf{Solution:} We have: $r = r \cos \theta$ and $y = r \sin \theta$. By the total derivative rule we have: 
	\begin{align*}
		& dx = \frac{\partial x}{\partial r} dr + \frac{\partial x}{\partial \theta} d \theta  \\
		& dy = \frac{\partial y}{\partial r} dr + \frac{\partial y}{\partial \theta} d \theta 
	\end{align*}
	and so: 
	\begin{align*}
		& dx = \cos \theta dr - r \sin \theta d \theta \\
		& dy = \sin \theta d r + r \cos \theta d \theta 
	\end{align*}
	and so from the properties of wedge products: 
	\begin{align*}
		dx \wedge dy & = \cos \theta r \cos \theta dr \wedge d \theta - r \sin \theta \sin \theta d \theta \wedge dr \\
			& = r \cos^2 \theta dr \wedge d \theta - r \sin ^2 \theta d \theta \wedge d r \\
			& = r \cos^2 \theta dr \wedge d \theta + r \sin ^2 \theta  d r \wedge d \theta \\
			& = r ( \cos^2 \theta + \sin ^2 \theta) dr \wedge d \theta \\
			& = r dr \wedge d \theta 
	\end{align*}
	Which is what we would expect from standard cal 2. 
\end{ex}

In general, if we have a system of equations: 
\begin{align*}
	& y_1 = a_{11} x_1 + a_{12} x_2 \\
	& y_2 = a_{21} x_1 + a_{22} x_2 
\end{align*}
and we collect the coefficients $a_{ij}$ into a matrix: 
\begin{align*}
	A := \begin{bmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{bmatrix}
\end{align*}
then we have: 
\begin{align*}
	d y_1 \wedge d y_2 = \det (A) dx_1 \wedge d x_2
\end{align*}
Which is also not very surprising. 

\begin{ex}
	Let $f: \R^2 \rightarrow \R^2$, $(x,y) \mapsto (u,v)$ according to: 
	\begin{align*}
		 u & = x^2 - y^2 \\
		v & = 2xy 
	\end{align*}
	Express $du \wedge dv$ in terms of $dx \wedge dy$. 
	\newline
	\textbf{Solution:} By the total derivative rule: 
	\begin{align*}
		& du = 2x dx - 2y dy \\
		& dv = dx dy + dy dx 
	\end{align*}
	and so, by the properties: 
	\begin{align*}
		du \wedge dv & = (2x dx - 2ydy ) \wedge (dx dy + dy dx) \\
					 & = dx dx \wedge (dx dy + dy dx) - dy dy \wedge (dx dy + 2ydx) \\
					 & = 4x^2 dx \wedge dy - 4y^2 dy \wedge dx \\
					 & = 4x^2 dx \wedge dy + 4y^2 dy \wedge dy \\
					 & = 4(x^2 + y^2) dx \wedge dy 
	\end{align*}
	Note that the quantity $ 4(x^2 + y^2) dx \wedge dy $ depends on how $f$ is defined, so the proper way to refer to this quantity is to say that $ 4(x^2 + y^2) dx \wedge dy $ is the \dfn{pull back} of $du \wedge dv$ via $f$. Mathematically, we would write: 
	\begin{align*}
		f^*(du \wedge dv) = 4(x^2 + y^2) dx \wedge dy 
	\end{align*}
\end{ex}

This example motivates the following rules for pull backs and wedge products. 

\begin{prop}
	Let $g$ be a function and let $\alpha$, $\omega$, and $\beta$ be differential forms. Then: 
	\begin{enumerate}[noitemsep]
		\item $g^*(\alpha \wedge \beta) = g^* \alpha \wedge g^* \beta$ 
		\item $g^*(f \omega) = (g^* f) (g^* \omega) $
	\end{enumerate}
\end{prop}



\begin{definition}[Exterior Derivative]
	We will define the exterior derivative in two steps: first for $0$-forms; then, we will generalise to $k$-forms. The \dfn{exterior derivative} of a smooth function $f$ is the differential $df \in \Omega^1 (U)$. With coordinates: 
	\begin{align*}
		df := \sum \frac{\partial f}{\partial x^i} dx^i
	\end{align*}
	Now let $k \geq 1$. Set $\omega = \sum_{I} a_I dx^I \in \Omega^k(U)$. Then the \dfn{exterior derivative} is defined as: 
	\begin{align*}
		d \omega & := \sum_{I} da_I \wedge dx^I \\
				 & = \sum_{I} \left( \sum_{J} \frac{\partial a_I}{\partial x^j } dx^j \right) \wedge dx^I \in \Omega^{k+1}(U)
	\end{align*}
\end{definition}
To make this clearer, let's do an example. Let $\omega$ be the 1-form $f dx + g dy$ on $\R^2$. Then: 
\begin{align*}
	d \omega & = df \wedge dx + df \wedge dy \\
				&  = (f_x dx + f_y dy ) \wedge dx + (g_x dx + g_y dy ) \wedge dy  \text{ (by definition)} \\
				& = (g_x - f_x) dx \wedge dy \text{ (by properties of wedge product) } 
\end{align*}
Here are two useful properties of the exterior derivative: 
\begin{prop}[Properties of the Exterior Derivative]
	Let $\alpha \in \Lambda^k(M)$, $\beta \in \Lambda^l(M)$. Let $a, b \in \R$. Then: 
	\begin{enumerate}[noitemsep]
		\item $d(a \alpha + \beta b) = a d \alpha + b d \beta$ (Linearity) 
		\item $d(\alpha \wedge \beta) = ( d \alpha) \wedge \beta + (-1)^k \alpha \wedge d \beta$ (Product rule) 
		\item $d(d \alpha) = 0$ 
	\end{enumerate}
\end{prop}

Here are some concrete examples of computing exterior derivatives.
\begin{ex}
	Let $\omega = ydx - z dy$. Compute the exterior derivative $d \omega$. \textbf{Solution:} 
	\begin{align*}
		d \omega = dy \wedge dx - dz \wedge dy 
	\end{align*}
\end{ex}

\begin{ex}
	Let $\omega = (x^2+y^2+z^2)(dx \wedge dy + dy \wedge dz)$. Compute the exterior derivative $d \omega$: 
	\begin{align*}
		d \omega & = (2x dx + 2y dy + dz dz) \wedge (dx \wedge dy + dy \wedge dz) \\
		& = 2x dx \wedge dy \wedge dz + 2z dz \wedge dx \wedge dy \\
		& = (2x + 2y) (dx \wedge dy \wedge dz) 
	\end{align*}
\end{ex}

\begin{ex}
	Let $\omega = \frac{xdy - ydx}{x^2 + y^2}$ be the angular form. Find the exterior derivative $d \omega$. 
	\newline
	\textbf{Solution:} Re-write the form as: 
	\begin{align*}
		(x^2 + y^2) \omega = xdy - ydx 
	\end{align*}
	Now take the exterior derivative of both sides: 
	\begin{align*}
		d ( (x^2 + y^2) \omega ) = d (xdy - ydx)
	\end{align*}
	Let's first simplify $d ( (x^2 + y^2) \omega )$: 
	\begin{align*}
		d ( (x^2 + y^2) \omega ) & = d(x^2 + y^2) \wedge \omega + (x^2 + y^2) d \omega \text{ (by the product rule) } \\
		& = (2x dx + 2y dy ) \wedge w + (x^2 + y^2) d \omega \\
		& = (2x dx + 2y dy ) \wedge \frac{x dy - y dx}{x^2 + y^2} - (x^2 + y^2) d \omega 
	\end{align*}
	Now expand out $(dx dx + 2y dy ) \wedge \frac{x dy - y dx}{x^2 + y^2}$: 
	\begin{align*}
		(2x dx + 2y dy ) \wedge \frac{x dy - y dx}{x^2 + y^2} & = 2x dx \wedge \left( \frac{xdy - ydx}{x^2 + y^2} \right) + 2y dy \wedge \left( \frac{x dy - y dx}{x^2 + y^2} \right) \\
		& = \frac{1}{(x^2+y^2)} \left[ 	2x^2 dx \wedge dy - 2xy dx \wedge dx + 2yx dy \wedge dy  - 2y^2 dy \wedge dx			\right ] \\
		& = \frac{1}{(x^2+y^2)} \left[ (2x^2 + 2y^2) dx \wedge dy \right] \\
		& = 2 (dx \wedge dy) 
	\end{align*} 
	And so we get: 
	\begin{align*}
		d( (x^2 + y^2) \omega ) = 2 dx \wedge dy + (x^2 + y^2) d \omega 
	\end{align*}
	Now we compute the exterior derivative $d(xdy - ydx)$: 
	\begin{align*}
		d(xdy - ydx)  = dx \wedge dy - dy \wedge dx = 2dx \wedge dy 
	\end{align*}
	And so: 
	\begin{align*}
		(x^2 + y^2) d \omega = 0 \iff d \omega = 0
	\end{align*}
	Since we are in the punctured disc and so $x^2 + y^2  > 0$. 
\end{ex}

There is a connection between the exterior derivative and the curl operation from advanced calculus. Precisely: let $\alpha$ be a general one-form of three variables be written as: 
\begin{align*}
	\alpha = Pdx + Q dy + R dz 
\end{align*}
Then, when taking the exterior derivative $d \alpha$ we recover the curl: 
\begin{align*}
	d \alpha & = d P \wedge dx + dQ \wedge dy + d R \wedge dz \\
	& = (R_y - Q_z) dy \wedge dz + (P_z - R_x) dz \wedge dx + (Q_x - P_y) dx \wedge dy \\
	& = \nabla \times F 
\end{align*}
\begin{definition}[Closed and Exact Forms]
	Let $\omega$ be a $k$-form on $U$. We say that $\omega$ is \dfn{closed} if $d\omega =0$. We say that $\omega$ is \dfn{exact} if $\exists$ a $(k-1)$-form $\tau$ such that $\omega = d \tau$. Every exact form is closed. 
\end{definition}


\subsection{Change of Variables for Integrals in $\R^n$}

\subsection{Integrating a $n$-Form on $M^n$ ($\idx{M}{} \omega$)} 
In this section, we will build up to the invariant Stokes' theorem. We will first start with line integrals, and how they can be written in terms of forms. 

\subsubsection{Line Integrals}
The objective is to compute the following object: 
\begin{align}
	\idx{\gamma}{} \omega	
\end{align}
where $\omega$ is a one-form and $\gamma$ is a path or curve. The general setup is as follows: 
\begin{enumerate}[noitemsep]
	\item Suppose that the variables in the differential one-form are $x_1, ..., x_n$. We will collect these into a vector $x:= (x_1, ..., x_n)$ and write the one-form $\omega$ as: 
	\begin{align*}
		\omega = \sum_{i=1}^n F_k dx_k 
	\end{align*}
	where $F_k$ is: 
	\begin{align*}
		F_k = F_k(x) = F_k(x_1, ..., x_n)
	\end{align*}
	\item There are two ways to describe $\gamma$: 
	\begin{enumerate}[noitemsep]	
		\item A system of parametric equations: $x_k := x_k(t)$ 
		\item In vector form: $x=x(t)$ where $t \in [a,b]$. 
	\end{enumerate}
\end{enumerate}
When $\gamma$ is just a standard path in $[a,b]$ (i.e., one that corresponds to standard Cal 2 integration), then we just have the standard definite integral when taking the \dfn{pull back} of $\omega$: 
\begin{align*}
	\idx{\gamma}{} \omega = \idx{a}{b} F(t) dt 
\end{align*}
You can think of the pull back as ``substituting'' $t$ into $F$. For the more general case, we \dfn{pull back} a differential form $\omega$ in $n$ variables $x_j$'s via $\gamma$ to get a differential form on \emph{one} variable $t$. This is denoted by $\gamma^* \omega$. You obtain it by the substitution: 
\begin{align*}
	x_j = x_j(t) 
\end{align*}
into $\omega$. So: 
\begin{align*}
	\omega = \sum_{k=1}^n F_k dx_k \text{ --PULL BACK: } \rightarrow \gamma^*(\omega) = \sum_{k=1}^n F_k(x(t)) dx_k (t) = \sum_{k=1}^n F_k(x(t))x'_k(t) dt  
\end{align*}
So, we can formally define a line integral in the general case. 

\begin{definition}[Line Integral -- Differential Forms]
	Let $\omega$ be a one-form given by $\omega = \sum_{k=1}^n F_k(x) dx_k$ and let $\gamma$ be a curve. Then, the \dfn{line integral} is defined as: 
	\begin{align}
		\idx{\gamma}{}\omega := \idx{a}{b} \gamma^* \omega 	
	\end{align}
	where $\gamma^* \omega = \sum_{k=1}^n F_k(x(t)) \frac{dx_k}{dt} dt$. 
\end{definition}
I find that all of this stuff is super confusing without clear examples, so here are some worked examples of line integrals of one-forms: 

\begin{ex}
	Compute the line integral: 
	\begin{align*}
		\idx{\gamma}{} xdy + y dz + zdx 
	\end{align*}
For the following three paths connecting the point $(0,0,0)$ to $(1,1,1)$: 
\begin{enumerate}[noitemsep]
	\item $\gamma = \alpha$: $(x,y,z) = (t,t,t)$ where $t \in [0,1]$. 
	\item $\gamma = \beta$: $(x,y,z) = (t,t^2, t^3)$ where $t \in [0,1]$. 
	\item $\gamma = \zeta$: $(x,y,z) = (t^2, t^4, t^6)$ where $t \in [0,1]$. 
\end{enumerate}
Computing the pullbacks gives us: 
\begin{enumerate}[noitemsep]
	\item $\alpha^* \omega = tdt + dtd + tdt = 3tdt$ 
	\item $\beta^* \omega = td(t^2) + t^2 d(t^3) + t^3 dt = (2t^2 + 3t^4 + t^3) dt $
	\item $\zeta^* \omega = (4t^5 + 6t^9 + 2t^7)dt$. 
\end{enumerate}
Carrying out the integration: 
\begin{align*}
	& \idx{\alpha}{} \omega = \idx{0}{1} 3tdt = 3/2 \\
	& \idx{\beta}{} \omega = \idx{0}{1} (2t^2 + 3t^4 + t^3)dt = 91/60 \\
	& \idx{\zeta}{} \omega = \idx{0}{1} (4t^5 + 6t^9 + 2t^7) = 91/60 
\end{align*}
\end{ex}

\begin{ex}
	Compute the line integral: 
	\begin{align*}
		\idx{\gamma}{} \omega := \idx{\gamma}{} \frac{xdy - ydx}{x^2 + y^2}
	\end{align*}
	where $\gamma$ is the path around the unit circle once in the anti-clockwise direction parameterised by $x = \cos t$ and $ y = \sin t$, $t \in [0, 2 \pi ]$. 
	\newline
	\newline
	\textbf{Solution:} Set: 
	\begin{align*}
		\omega := \frac{xdy - ydx}{x^2 + y^2}
	\end{align*}
	Compute the pullback: 
	\begin{align*}
		\gamma^* \omega & = \frac{x(t) dy(t) - y(t) dx(t)}{(x(t))^2 + (y(t))^2} \\
		& = \frac{\cos (t) d ( \sin(t)) - \sin (t) d ( \cos(t)) }{( \cos(t))^2 + ( \sin(t))^2} \\
		& = \frac{\cos^2(t) + \sin^2(t)}{\cos^2(t) + \sin^2(t)} \\
		& = 1
	\end{align*}
	and so the integral becomes: 
	\begin{align*}
		\idx{\gamma}{} \omega = \idx{0}{2 \pi} dt = 2 \pi 
	\end{align*}
\end{ex}

\subsubsection{Surface Integrals}
\subsubsection{Generalised Stokes' Theorem}

\section{Curves}
There are two subsets of differential geometry: classical differential geometry and global differential geometry. The objective of \dfn{classical differential geometry} is to study the local properties of curves and surfaces. The objective of \dfn{global differential geometry} is to study the influence of local properties on global behaviour. 
\subsection{Definitions}

\begin{definition}[Parameterised Differentiable Curve] 
	A \dfn{parameterised differentiable curve} is a differentiable map $ \alpha: I \rightarrow \R^3$ of an open interval $I = ]a,b[$ of the real line $\R$ into $\R^3$. The image of $\alpha$ is called the \dfn{trace} of $\alpha$. 
\end{definition}

Some examples of parameterised curves include: 
\begin{itemize}[noitemsep]
	\item The helix: $\alpha(t) = (a \cos (t), a \sin(t), bt)$ for $t \in \R$. 
	\item The map $\alpha: \R \rightarrow \R^2$, $t \in \R$, is a parameterised differentiable curve. 
\end{itemize}

\begin{definition}[Norm on $\R^3$]
	Let $u = (u_1, u_2, u_3) \in \R^3$. The \dfn{norm} of $u$ is: 
	\begin{align*}
		|| u || := \sqrt{ u_1^2 + u_2^2 + u_3^3} 
	\end{align*}
\end{definition}

\begin{definition}[Inner Product on $\R^3$] 
	Let $u = (u_1, u_2, u_3)$ and $v= (v_1, v_2, v_3)$ belong to $\R^3$ and let $\theta \in [0, \pi]$ be the angle formed between $u,v$. The \dfn{inner product} is defined by: 
	\begin{align}
		u \cdot v := || u || ||v|| \cos (\theta) 	
	\end{align}
\end{definition}
It satisfies the following properties: 
\begin{enumerate}[noitemsep]
	\item If $u$, $v$ are non-zero, then $u \cdot v = 0$ $\iff$ $u \perp v$. 
	\item $u \cdot v = v \cdot u$. 
	\item $\lambda (u \cdot v ) = \lambda u \cdot v =  u \cdot \lambda v$. 
	\item $u ( v + w) = u \cdot v + u \cdot w$. 
\end{enumerate}
If we have made a choice of basis, then we can formulate the dot product in terms of the components of the vectors as: 
\begin{align}
	u \cdot v = u_1 v_1 + u_2 v_2 + u_3 v_3 	
\end{align}

\subsubsection{Regular Curves and Arclength}
In differential geometry, it is \underline{essential} that our curves have a tangent line at every point. This motivates the following definition. 

\begin{definition}[Regular Curve]
	A parameterised differentiable curve $\alpha: I \rightarrow \R^3$ is \dfn{regular} if $\alpha' (t) \neq 0$ $\forall t \in I$. 
\end{definition}

\begin{definition}[Arc length]
	Given $t_0 \in I$, the \dfn{arc length} of a regular parameterised curve $\alpha: I \rightarrow \R^3$ from $t_0$ to $t$ is defined to be: 
	\begin{align*}
		s(t) := \idx{t_0}{t} | a'(t) | dt 
	\end{align*}
	where
	\begin{align*}
		| \alpha' (t) | := \sqrt{(x'(t))^2 + (y'(t))^2 + (z'(t))^2}
	\end{align*}
	Since we only restrict our attention to regular surfaces, $a'(t) \neq 0$ for all $t$, and so the arlength function is a differentiable function of $t$ and $ds/dt = |a'(t)|$ (by the Fundamental Theorem of Calculus). Arc length parameterisations make life simpler. 
\end{definition}

\subsubsection{The Vector Product in $\R^3$}

\begin{definition}[Vector Product]
	Let $u,v \in \R^3$. Then, the \dfn{vector product} of $u,v$ is the unique vector $u \wedge v$ in $\R^3$ characterised by: 
	\begin{align*}
		(u \wedge v) \cdot w = \text{det}(u,v,w)\ \text{  } \forall w \in \R^3
	\end{align*}
	this is more commonly known as:
	\begin{align*}
		u \wedge v = \text{det} \begin{bmatrix}
			\hat{i} & \hat{j} & \hat{k} \\
			u_1 & u_2 & u_3 \\
			v_1 & v_2 & v_3 
		\end{bmatrix}
	\end{align*}
	where $\hat{i}, \hat{j}, \hat{k}$ are the standard basis vectors in $\R^3$. 
\end{definition}

\underline{Properties of the Vector Product} 
\begin{enumerate}[noitemsep]
	\item (Anti-Commutativity): $u \wedge v = -v \wedge u$. 
	\item (Linear Dependence): $\forall$ $\alpha$, $\beta$ $\in \R$: 
	\begin{align*}
		(\alpha u + \beta v) \wedge v = \alpha u \wedge v + \beta w \wedge v
	\end{align*}
	\item $u \wedge v = 0$ $\iff$ $u$ and $v$ are linearly dependent. 
	\item $(u \wedge v) \cdot u =0$, $(u \wedge v) \cdot v =0$ (this implies that the vector product is normal to the plane generated by $u$ and $v$). 
\end{enumerate}

\subsection{Frenet-Serret Frame} % TO DO: Make this section more comprehensible, just a list of definitions for now. 

\begin{definition}[Curvature]
	Let $\alpha: I \rightarrow \R^3$ be a curve parameterised by arclength $s \in I$. The number $|| \alpha''(s) || = \kappa (s)$ is called the \dfn{curvature} of $\alpha$ at $s$. 
\end{definition}
It's straightforward to check that $\kappa(s) = 0 \iff \alpha(s) = us + v$ (i.e., the curve is actually a straight line). When $\kappa(s) \neq 0$, the \dfn{unit normal} $n(s)$ in the direction $\alpha''(s)$ is well-defined and is given by: 
\begin{align*}
	\alpha''(s) := \kappa(s) \cdot n(s)
\end{align*}
The orthogonality of $n(s)$ to $\alpha'(s)$ can be verified by differentiating both sides of $\alpha'(s) \cdot \alpha'(s) = 1$ since $||\alpha'(s)|| =1$. 

\begin{definition}[Osculating Plane at $s$]
	The \dfn{osculating plane} at $s$ is the plane determined by the unit tangent and normal vectors, $\alpha'(s)$, and $n(s)$. 
\end{definition}

\begin{definition}[Binormal Vector at $s$, $b(s)$] 
	The \dfn{binormal vector} as $s$ is defined as $t(s) \wedge n(s)$, where $t(s)$ is the unit tangent at $s$. The magnitude of this vector, $||b(s)||$, measures how rapidly the curve pulls away from the osculating plane at $s$ in a neighbourhood of $s$. 
\end{definition}

\begin{definition}[Torsion]
	Let $\alpha: I \rightarrow \R^3$ be a curve parameterised by arclength $s$ such that $\alpha''(s) \neq 0$, $s \in I$. The number $\tau(s)$ defined by $b'(s) := \tau(s)  n(s)$ is called the \dfn{torsion} of $\alpha$ at $s$. We have the following useful characterisation: 
	\begin{align*}
		\alpha \text{ is a plane curve} \iff \tau \equiv 0
	\end{align*}
	Thus, torsion measures how much a curve \emph{fails} to be a plane curve. 
\end{definition}

Collecting the orthogonal unit vectors $t(s), n(s), b(s)$ gives us the \dfn{Frenet Trihedron} at $s$. Using the above definitions gives us the \dfn{Frenet Formulae}, which is a set of differential equations: 
\begin{align}
	& t' = \kappa n \\
	& n' = - \kappa t - \tau b \\
	& b' = \tau n 	
\end{align}
\begin{itemize}[noitemsep]
	\item The $tb$ plane is called the \dfn{rectifying plane} 
	\item The $nb$ plane is called the \dfn{normal plane} 
	\item $\kappa$ and $\tau$ completely describe a curve's behaviour. 
	\item Bending $\sim$ curvature; twising $\sim$ torsion. 
\end{itemize}
The Frenet-Serret frame can be concisely expressed as a skew-symmetric matrix: 
\begin{align}
	\begin{bmatrix}
		T' \\
		N ' \\
		B' 
	\end{bmatrix}	 = \begin{bmatrix}
		0 & \kappa & 0 \\
		- \kappa & 0 & \tau \\
		0 & - \tau & 0 
	\end{bmatrix} \cdot \begin{bmatrix}
		T \\
		N \\
		B
	\end{bmatrix}
\end{align}

\begin{theorem}[Fundamental Theorem of the Local Theory of Curves]
	Given differentiable functions  $\kappa(s) > 0$ and $\tau(s)$, $s \in I$, there exists a regular parameterised curve $\alpha: I \rightarrow \R^3$ such that $s$ is the arclength, $\kappa(s)$ is the curvature, and $\tau(s)$ is the torsion of $\alpha$. Moreover, any other curve $\widetilde{\alpha}$ satisfying the same conditions differ from $\alpha$ by a \underline{rigid motion}. 
\end{theorem}

\begin{definition}[Rigid Motion]
	A \dfn{rigid motion} means that $\exists$ an orthogonal map $\rho$ of $\R^3$ with positive determinant and a vector $c$ such that $\widetilde{\alpha} = \rho \circ \alpha + c$. 
\end{definition}

Without loss of generality, we can assume curves to be parameterised by arclength, since we can always re parameterise a parameterised curve by arclength: 

Let $\alpha: I \rightarrow \R^3$ be a regular parameterised curve. Then, it is possible to obtain a curve $\beta: J \rightarrow \R^3$ that is parameterised by arc length with the same trace as $\alpha$: 
\begin{align*}
	s = s(t) = \idx{t_0}{t} | \alpha'(t) | dt
\end{align*}
where $t, t_0 \in I$. 

\subsection{Global Properties of Curves}

\subsubsection{The Isoparametric Inequality}
This is related to the following isoparametric question: 
\begin{center}
	\textbf{\underline{Q}}: Of all the simple closed curves in the plane with a given length, which bounds the largest area? 
\end{center}
We will use the following formula for the area $A$ bounded by a positively oriented simple closed curve $\alpha(t) = (x(t), y(t))$:
\begin{align*}
	A = - \idx{a}{b} y(t) x'(t) dt = \idx{a}{b} x(t)y'(t) dt = \frac{1}{2} (xy' - yx') dt 
\end{align*}

\begin{theorem}[The Isoparametric Inequality]
	Let $C$ be a simple closed plane curve with length $\ell$ and let $A$ be the area of the region bounded by $C$. Then: 
	\begin{align}
		\ell^2 - 4 \pi A \geq 0 	
	\end{align}
	where equality holds $\iff$ $C$ is a circle. 

\end{theorem}

\subsubsection{Cauchy Crofton Formula}

\begin{theorem}[Cauchy Crofton Formula]
	Let $C$ be a regular plane curve with length $\ell$. The measure of the set of straight lines, counted with multiplicities (\dfn{multiplicity} is the number of intersection points of a line with $C$), which meet $C$ is equal to $2 \ell$. 
\end{theorem}

\begin{definition}[Rigid Motion in $\R^2$]
	A \dfn{rigid motion} in $\R^2$ is a map $F: \R^2 \rightarrow \R^2$ given by $(\overline{x}, \overline{y}) \rightarrow (x,y)$, where: 
	\begin{align*}
		& x = a + \overline{x} \cos(\varphi) - \overline{y} \sin( \varphi) \\
		& y = b + \overline{x} \sin( \varphi) + \overline{y} \cos (\varphi) 
	\end{align*}
\end{definition}

\begin{prop}
	Let $f(x,y)$ be a continuous function defined in $\R^2$. For any set $S \subseteq \R^2$, define the \dfn{area $A$ of $S$} by: 
	\begin{align}
	A(S) := \iint_{S} f(x,y) dx dy 	
	\end{align}
	Assume that $A$ is invariant under rigid motions; that is, if $S$ is a set and $\overline{S} = F^{-1}(S)$, where $F$ is a rigid motion, then if: 
	\begin{align*}
		A (\overline{S} ) = \iint_{\overline{S}} f(\overline{x}, \overline{y}) d \overline{x} d \overline{y} 	= \iint_{S} f(x,y) dx dy 	= A(S)
	\end{align*}
	Then, $f(x,y)$ is a constant. 
\end{prop}


\section{Surfaces}

\subsection{Definitions}

\textbf{Motivation:} we want to define a regular surface to be something that is nice enough for us to extend the usual notions of calculus to. 


\begin{definition}[Regular Surface]
	A subset $S \subseteq \R^3$ is called a \dfn{regular surface} if, $\forall$ $p \in S$, there exists a neighbourhood $V \subseteq \R^3$ and a map $\mathbb{X}: U \rightarrow V \cap S$ of an open set $V \subseteq \R^2$ onto $V \cap S \subseteq \R^3$ for which the following conditions hold: 
	\begin{enumerate}[noitemsep]
		\item $\mathbb{X}$ is differentiable; that is, if we write 
		\begin{align*}
			\mathbb{X}(u,v) = (x(u,v), y(u,v), z(u,v)) 
		\end{align*} 
		for $(u,v) \in U$, then the functions $x(u,v)$, $y(u,v)$ and $z(u,v)$ have continuous partial derivatives of all orders in $U$. 
		\item $\mathbb{X}$ is a \dfn{homeomorphism}: there exists an inverse $\mathbb{X}^{-1}: V \cap S \rightarrow U$, which is continuous. 
		\item (Regularity Condition): $\forall q \in U$, the differential $d$x$_q: \R^2 \rightarrow \R^3$ is bijective. 
	\end{enumerate}
	Then, the mapping $\mathbb{X}$ is called a \dfn{parameterisation}  or a \dfn{system of local coordinates} in a neighbourhood of $p$. The neighbourhood $V \cap S$ of $p$ is called a \dfn{coordinate neighbourhood}. 
\end{definition}


\subsection{Regular Surfaces}
\begin{ex}[The Unit Sphere is a Regular Surface] 
	The Unit Sphere is a regular surface. It's parametrised by: 
	\begin{align*}
		S^2 := \{ (x,y,z) \in \R^2\ |\ x^2 + y^2 + z^2 = 1 \} 
	\end{align*}
\end{ex}
In the textbook, they check all three conditions from the above definition. Since this can be quite exhausting, we want some propositions that simplify the task of determining if a surface is regular or not. This is the aim of this section. 

\begin{prop}
	If $f: U \subseteq \R^2 \rightarrow \R$, $U$ open, is a differentiable, then the graph of $f$, that is, the subset of $\R^3$ given by $(x,y, f(x,y))$ for $(x,y) \in U$, is a regular surface. 
\end{prop}

Before introducing the second proposition, we will first need to define critical points, critical values, and regular values for differentiable maps. 

\begin{definition}[Critical Point] 
	Given a differentiable map $F: U \subseteq \R^n \rightarrow \R^m$ defined in an open set $U \subseteq \R^n$, we say that $p \in U$ is a \dfn{critical point} of $F$ id the differential d$F_p: \R^n \rightarrow \R^m$ is not a surjective mapping. The image $F(p) \in \R^m$ of a critical point is called a \dfn{critical value} of $F$. A point $\R^m$ which is not a critical value is called a \dfn{regular value}. 
\end{definition}

The justification for the next proposition comes from the inverse function theorem. 

\begin{prop}
	If $f: U \subseteq \R^3 \rightarrow \R$ is a differentiable function and $a \in f(U)$ is a regular value of $f$, then $f^{-1}(a)$ is a regular surface in $\R^3$. 
\end{prop}

\begin{ex}[Ellipsoid] 
		The ellipsoid is given by: 
		\begin{align*}
			\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1
		\end{align*}
		Since it is the set $f^{-1}(0)$ where 
		\begin{align*}
			f(x,y,z) = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1
		\end{align*}
		and $f$ is a differentiable function and $0$ is a regular value of $f$. 
\end{ex}

\begin{definition}[Connected] 
	A surface $S \subseteq \R^3$ is \dfn{connected} if any two of its points can be joined by a continuous curve in $S$. 
\end{definition}

The next proposition is a very useful property that follows from the intermediate value theorem: 

\begin{definition}
	If $f: S \subseteq \R^3 \rightarrow \R$ is a non-zero continuous function defined on a connected surface $S$, then $f$ does not change sign on $S$. 
\end{definition}



\subsection{Differentiable Functions on Surfaces}

\subsection{Tangent Plane}

The third condition of a regular surface guarantees that for any fixed point $p \in S$, the set of tangent vectors to the parameterised curves of $S$ passing through $p$ constitutes a plane. 

\begin{prop}
	Let $\mathbb{X}: U \subseteq \R^2 \rightarrow S$ be a parameterisation of a regular surface $S$ and let $q \in U$. The vector subspace of dimension 2: 
	\begin{align}
		\text{d}x_q (\R^2) \subseteq \R^3 	
	\end{align}
	coincides with the set of tangent vectors to $S$ at $\mathbb{X}(q)$. 
\end{prop}
This plane does not depend on the parameterisation $\mathbb{X}$ and it is called the \dfn{tangent plane} to $S$ at $p$ and is denoted by $T_p(S)$. A choice of parameterisation $\mathbb{X}$ induces a basis on $T_p(S)$: 
\begin{align*}
	\{ (\partial \mathbb{X}/ \partial u)(q), ( \partial \mathbb{X}/ \partial v)(q) \} 
\end{align*}


The next proposition states that a map between two regular surfaces induces a map between the tangent planes, which we can think of as the differential of the map. 

\begin{prop}
	Let $S_1$, $S_2$ be regular surfaces and let $\varphi: V \subseteq S_1 \rightarrow S_2$ be a differentiable mapping of an open set $V$ of $S_1$ into $S_2$. Then, tangent vectors $w \in T_p(S_1)$ are the velocity vectors $\alpha'(0)$ of a differentiable parameterised curve $\alpha: ]-\varepsilon, \varepsilon[ \rightarrow V$ with $\alpha(0) = p$. If we define $\beta:= \varphi \circ \alpha$, then $\beta '(0)$ is a vector of $T_{\varphi(p)}(S_2)$. Given a $w$, the vector $\beta'(0)$ does not depend on the choice of $\alpha$ and the map d$\varphi_p: T_p(S_1) \rightarrow T_{\varphi(p)}(S_2)$  defined by d$\varphi_p(w) = \beta'(0)$ is linear. 
\end{prop}
Before moving onto the next proposition, we first need to define what a local diffeomorphism is. The aim is to build up to a generalisation of the standard inverse function theorem from calculus. 

\begin{definition}[Local Diffeomorphism] 
	A mapping $\varphi: U \subseteq S_1 \rightarrow S_2$ is called a \dfn{local diffeomorphism} at $p \in U$ if there is a neighbourhood $V \subseteq U$ of $p$ such that $\varphi|_U$ is a diffeomorphism onto an open set $\varphi(V) \subseteq S_2$. 
\end{definition}

\begin{prop}
	If $S_1$ and $S_2$ are regular surfaces and $\varphi: U \subseteq S_1 \rightarrow S_2$ is a differentiable mapping of an open set $U \subseteq S_1$ such that the differential d$\varphi_p$ of $\varphi$ at $p \in U$ is an isomorphism, then $\varphi$ is a local diffeomorphism at $p$. 
\end{prop}

For any point on a regular surface, we can find two unit normal vectors. By fixing a parameterisation $\mathbb{X}: U \subseteq \R^2 \rightarrow S$ for $p \in S$, we can make a definite choice of a unit normal at each point $q \in \mathbb{X}(U)$ by the following rule: 
\begin{align}
	N(q) := \frac{\mathbb{X}_u \wedge x_v}{|| x_u \wedge x_v ||} (q) 	
\end{align}
This gives us a differentiable map $N: \mathbb{X}(U) \rightarrow \R^3$. 



\subsection{First Fundamental Form: Area}
\textbf{Motivation:} the natural inner product on $\R^3$ induces on each regular surface $S \subseteq \R^3$'s tangent plane $T_p(S)$ an inner product, $\langle \cdot, \cdot \rangle_p$. The aim of the First Fundamental Form is to express how a surface inherits the natural inner product of $\R^3$. This allows us to make metric measurements of the surface, such as lengths of curves, angles of tangent vectors, and areas of regions without referring to the ambient space in which they reside.  


\begin{definition}[First Fundamental Form]
	Let $w_1$, $w_2 \in T_p(S) \subseteq \R^3$. Then, the quadratic form given by $I_p: T_p(S) \rightarrow \R$: 
	\begin{align}
		I_p(w) := \langle w, w \rangle_p = || w ||^2 > 0 	
	\end{align}
	is called the \dfn{First Fundamental Form} of the regular surface $S \subseteq \R^3$ at $p \in S$. 
\end{definition}

\subsubsection{Deriving the First Fundamental Form Given a Basis and a Parameterisation}

Let $\mathbb{X}(u,v)$ be a parametrisation. We will now express the first fundamental form in the basis $\{ \mathbb{X}_u, \mathbb{X}_v \}$ associated to a parameterisation $\mathbb{X}(u,v)$ at $p$. Recall that a tangent vector $w \in T_p(S)$ is equivalent to a tangent vector to a parameterised curve $\alpha(t) = \mathbb{X}(u(t), v(t) ) $ for $t \in ]-\varepsilon, + \varepsilon[$ for which $p = \alpha(0) = \mathbb{X}(u_0, v_0)$. 

From the definition of the first fundamental form, we have:
\begin{align*}
	 I_p(\alpha'(0)) & = \langle \alpha'(0), \alpha'(0) \rangle_p \\
	 				 & = \langle \mathbb{X}_u u' + \mathbb{X}_v v', \mathbb{X}_u u' + \mathbb{X}_v v' \rangle_p \\
	 				 & = \langle \mathbb{X}_u, \mathbb{X}_u \rangle_p (u')^2 + 2 \langle \mathbb{X}_u, \mathbb{X}_v \rangle u' v' + \langle \mathbb{X}_v, \mathbb{X}_v \rangle_p (v')^2
\end{align*}
If we define 
\begin{align*}
	& E(u_0, v_0) := \langle \mathbb{X}_u, \mathbb{X}_u \rangle_p  \\
	& F(u_0, v_0) := \langle \mathbb{X}_u, \mathbb{X}_v \rangle_p  \\
	& G(u_0, v_0) := \langle \mathbb{X}_v, \mathbb{X}_v \rangle_p 
\end{align*}
then the first fundamental form can be expressed as: 
\begin{align*}
	I_p = E (u')^2 + 2 F u' v' + G(v')^2
\end{align*}

\subsubsection{Examples of First Fundamental Forms}

\begin{enumerate}[noitemsep]
	\item Recall that the \dfn{plane} going through $p_0 = (x_0, y_0, z_0)$ containing the orthonormal vectors $w_1 = (a_1, a_2, a_3)$ and $w_2 = (b_1, b_2, b_3)$ is given by: 
	 \begin{align*}
		\mathbb{X}(u,v)  = p_0 + uw_1 + vw_2 
	\end{align*}
	for $(u,v) \in \R^2$. Then, $E = 1$, $F=0$, and $G=1$. 
	\item The \dfn{cylinder} over the circle $x^2 + y^2 = 1$ parameterised by $\mathbb{X} (u,v) = (\cos (u) , \sin(v), v) $ where $u \in ]0, 2 \pi[$ and $v \in \R$. Then: $E = \sin^2 (u) + \cos^2(u) =1$, $F =0$, and $G=1$. 
	\item The \dfn{Helicoid} is given by: $\mathbb{X}(u,v) := (v \cos (u), v \sin(u) au)$. $ u \in ]0, 2 \pi[$, $v \in \R$. The first fundamental form is given by: $ E = v^2 + a^2$, $F(u,v) = 0$, and $G(u,v) = 1$. 
\end{enumerate}
We can express arclength in terms of the terms of the functions of the first fundamental form. Let $s$ be an arclength-parameterised curve $\alpha: I \rightarrow s$. Then, the arc-length is: 
\begin{align*}
	s(t) = \idx{0}{t} | \alpha'(t) | dt = \idx{0}{t} \sqrt{I(\alpha'(t))} dt
\end{align*}
Substituting in the derivation gives us: 
\begin{align*}
	s(t) = \idx{0}{t} \sqrt{E(u')^2  + 2Fu' v' + G(v')^2} dt 
\end{align*}
We can also represent angles of intersections of parameterised curves using the coefficients of the first fundamental form. Let $\alpha: I \rightarrow S$ and $\beta: I \rightarrow S$ be two parameterised curves. The angle $\theta$ at which they intersect at $t=t_0$ is given by: 
\begin{align} 
\cos(\theta) = \frac{\langle a'(t_0), \beta'(t_0) \rangle}{|| \alpha'(t_0) || || \beta'(t_0) ||}
\end{align} 
In terms of the coefficients of the first fundamental form, we have: 
\begin{align*}
	\cos(\theta) = \frac{\langle x_u, x_v \rangle}{||x_u|| ||x_v || } = \frac{F}{\sqrt{EG}}
\end{align*}
A special type of parameterisation is called an \dfn{orthogonal parameterisation}, which is a parameterisation where the coordinate curves of a parameterisation are orthogonal. By the above, this happens if and only if $F(u,v) = 0$ for all $u,v \in S$. Moreover, from the arc length formula, an \dfn{element of arclength} is given by: 
\begin{align*}
	ds^2 = E du^2 + 2F du dv + G dv^2 
\end{align*}
One final classic example of computing first fundamental forms is that of a sphere. If we parameterise a sphere as: 
\begin{align*}
	\mathbb{X}(\theta, \varphi) = ( \sin \theta \cos \varphi, \sin \theta, \sin \varphi, - \sin \theta ) 
\end{align*}
Then, the coefficients of the first fundamental form become: 
\begin{align*}
	& E(\theta, \varphi) = 1 \\
	& F(\theta, \varphi) = 0 \\
	& G(\theta, \varphi) = \sin^2 (\theta) 
\end{align*}
Then, for a vector $w \in T_p(S)$ at the point $p$ with the coordinates based on the basis associated to the parametrisation $\mathbb{X}( \theta, \varphi)$, we write: 
\begin{align*}
	w = a \mathbb{X}_\theta + b \mathbb{X}_\varphi 
\end{align*}
and so
\begin{align*}
	||w||^2 = I(w) = Ea^2 + 2Fab + Gb^2 = a^2 + b^2 \sin^2 \theta 
\end{align*}
We can use the first fundamental form to compute areas. 
\begin{definition}[Area]
	Let $R \subseteq S$ be a bounded region of a regular surface contained in the coordinate neighbourhood of the parameterisation $\mathbb{X}: U \subseteq \R^2 \rightarrow S$. Then, the positive number: 
	\begin{align*}
		A(R) := \iint_Q || \mathbb{X}_u \wedge \mathbb{X}_v || du dv 
	\end{align*}
	where $Q = \mathbb{X}^{-1}(R)$ is called the \dfn{area} of $R$. This is equivalent to, in terms of the first fundamental form: 
	\begin{align*}
		& = \iint_Q \sqrt{EG - F^2} du dv 
	\end{align*}
\end{definition}

\section{The Gauss Map}
\textbf{Motivation:} try to measure how rapidly a surface $S$ pulls away from the tangent plane $T_p(S)$ in a neighbourhood of a point $p \in S \leftrightarrow$ measuring the rate of change at $p$ of a unit normal vector field $N$ on a neighbourhood of $p$. This gives rise to a linear map on $T_p(S)$ that is self-adjoint. This map happens to give us a lot of information about local properties of the surface $S$ at $p$. 

\subsection{The Definition of the Gauss Map and its Fundamental Properties}

\begin{itemize}[noitemsep]
	\item $N$ is said to be a \dfn{differentiable field of unit normal vectors on} an open set $V \subseteq S$ if $N: V \rightarrow \R^3$ is a differentiable map which associates to each $q \in V$ a unit normal vector at $q$. 
	\item A regular surface $V$ is called \dfn{orientable} if it admits a differentiable field of unit normal vectors defined on the whole surface. 
	\begin{itemize}[noitemsep]
		\item The Möbius strip is an example of a non-orientable surface. 
		\item The choice of such a field $N$ is called an \dfn{orientation} of $S$. 
		\item Every surface is locally orientable. 
		\item Orientation is a global property in the sense that it involves the \emph{whole} surface. 
	\end{itemize}
\end{itemize}

The Gauss map is the map which assigns unit normals to points on surfaces. We derived this map in homework 1. 

\begin{definition}[Gauss Map] 
	Let $S \subseteq \R^3$ be a surface with orientation $N$. The map $N: S \rightarrow \R^3$ takes its values in the unit sphere: 
	\begin{align}
		S^2 := \{ (x,y,z) \in \R^3\ |\ x^2 + y^2 + z^2 = 1 \} 	
	\end{align}
	This map $N: S \rightarrow S^2$ as defined is called the \dfn{Gauss Map} of $S$. 
\end{definition}
The differential induced by the Gauss Map, d$N_p: T_p(S) \rightarrow T_{N(p)}(S)$ , is a linear map. Restricting the map to a parameterised curve $\alpha(t)$ in $S$ provides for us a measure of how $N$ pulls away from $N(p)$ in a neighbourhood of $p$. For curves, this information is encoded in the curvature, a scalar. For surfaces, the ``notion'' of curvature is encoded as a linear map. 

Here are several examples of what $dN$ would be for some surfaces. 
\begin{enumerate}[noitemsep]
	\item The \dfn{plane} has zero ``curvature.'' Parameterise this plane by $ax + by + cz + d =0$. Then, the unit normal vector is given by: 
	\begin{align*}
		N = \frac{(a,b,c)}{\sqrt{a^2 + b^2 + c^2}}
	\end{align*}
	and is thus a constant. This means that d$N =0$. 
	\item The \dfn{unit sphere} is parameterised by: 
	\begin{align*}
		S^2 = \{ (x,y,z) \in \R^3\ |\ x^2 + y^2 + z^2 = 1 \}
	\end{align*}
	Fix an orientation on $S^2$ by choosing $N= (-x, -y, -z)$. Then, d$N_p(v) = -v$ for $p \in S^2$, $v \in T_p(S^2)$. 
	\item The \dfn{cylinder over the unit circle} is parameterised by:  
	\begin{align*}
		C = \{ (x,y,z) \in \R^3\ |\ x^2 + y^2 = 1 \} 
	\end{align*}
	Fix an orientation by choosing $N= (-x, -y, 0)$. For a $v \in T_p(C)$, there are two cases: 
	\begin{enumerate}[noitemsep]
		\item If $v$ is tangent to the cylinder and parallel to the z-axis, then d$N(v) = 0 = 0v$. 
		\item If $v$ is tangent to the cylinder and parallel to the $xy$-plane, then $dN(w) = -w$. 
	\end{enumerate}
	$v$ and $w$ are eigenvectors of $dN$ with eigenvalues 0 and -1, respectively. 
	\item \dfn{Hyperbolic Paraboloid}: analyse the point $p=(0,0,0)$ of the hyperbolic paraboloid. Parameterise it by: 
	\begin{align*}
		\mathbb{X}(u,v) = ( u, v, v^2 - u^2) 
	\end{align*}
	The normal vector is given by: 
	\begin{align*}
		N = \left( 	\frac{u}{\sqrt{u^2 + v^2 + 1/4}}, \frac{-v}{\sqrt{u^2 + v^2 + 1/4}}	, \frac{1}{2 \sqrt{u^2 + v^2 + 1/4}}		\right) 
	\end{align*}
	and so at $p$, d$N_p (u'(0), v'(0), 0) = (2u'(0), -2v'(0), 0)$ meaning that $(1,0,0)$ and $(0,1,0)$ are eigenvectors of $dN_p$ with eigenvalues 2 and -2 respectively. 
\end{enumerate}
Before introducing the second fundamental form, we need to first define an self-adjoint map. 

\begin{definition}[Self-Adjoint]
	We say that a linear map $A: V \rightarrow V$ is \dfn{self-adjoint} if $\langle Av, w \rangle = \langle v, Aw \rangle$ $\forall$ $v, w \in V$. 
\end{definition}

The following proposition is useful since it allows us to associate d$N_p$ to a quadratic form $Q$ in $T_p(S)$, which will be important for the second fundamental form. The quadratic form will be given by: 
\begin{align*}
	W(v) = \langle dN_p(v), v \rangle 
\end{align*}
for $v \in T_p(S)$. 


\begin{prop}
	The differential of the Gauss Map, d$N_p: T_p(S) \rightarrow T_p(S)$, is a self-adjoint linear map. 
\end{prop}

\begin{definition}[The Second Fundamental Form]
	The quadratic form $II_p$ defined in $T_p(S)$ given by $II_p(v) = - \langle dN_p(V), v \rangle$ is called the \dfn{second fundamental form} of $S$ at $p$. 
\end{definition}

\begin{definition}[Normal Curvature]
	Let $C$ be a regular surface in $S$ passing through $p \in S$, $\kappa$ the curvature of $C$ at $p$, and $\cos \theta = \langle n, N \rangle$ where $n$ is the normal vector to $C$ and $N$ is the normal vector to $S$ at $p$. Then, the number $k_n := k \cos \theta$ is called the \dfn{normal curvature} of $C \subseteq S$ at $p$. 
\end{definition}
Thus, $k_n$ represents the length of the projection of the vector $kn$ over the normal to the surface at the point $p \in C$. 

\begin{prop}[Meusnier] \label{curve} 
	All of the curves lying on a surface $S$ with the same tangent line at a given point $p \in S$ have the same normal curvatures. 
\end{prop}
\begin{itemize}[noitemsep]
	\item Gives meaning to the notion of ``normal curvature along a given direction at $p$''. 
	\item \dfn{Normal section of $S$ at $p$}: given a unit vector $v \in T_p(S)$, the intersection of $S$ with the plane containing $v$ and $N(p)$ is called the \dfn{normal section of $S$ at $p$} along $v$. 
	\item The curvature of a curve is equal to the absolute value of the normal curvature along $v$ at $p$, where $v$ is the tangent vector of the curve at $p$. 
	\item So, Prop. \ref{curve} is saying that the absolute value of the normal curvature at $p$ of a curve $\alpha(s)$ is equal to the curvature of the normal section of $S$ at $p$ along $\alpha'(0)$. 
\end{itemize}
Examples of second fundamental forms for surfaces: 
\begin{enumerate}[noitemsep]
	\item \dfn{Plane}: all normal sections are straight lines. So, all normal curvatures are zero. Thus, the second fundamental form is identically equal to zero at all points $\leftrightarrow$ d$N \equiv 0$. 
	\item \dfn{Sphere $S^2$}: Choose an orientation $N$. The normal sections through a point $p \in S^2$ are circles with radius $1$. Thus, all normal curvatures are equal to $1$, and so the second fundamental form is $II_p(v) = 1$ $\forall p \in  S^2$, $v \in T_p(S)$, $|v| =1$. 
	\item \dfn{Cylinder}: normal sections vary from a circle perpendicular to the cylinder's axis to straight lines parallel to the axis, which means that normal curvature varies from 1 to 0. 
\end{enumerate}

\begin{definition}[Maximum Normal Curvature and Minimum Normal Curvature]
	The \dfn{maximum} \dfn{normal} \dfn{curvature} $k_1$ and the \dfn{minimum normal curvature} $k_2$ are called the principle curvatures at $p$; the corresponding directions, that is, the directions given by the eigenvectors $\{ \hat{e_1}, \hat{e_2} \}$, are called the \dfn{principal directions} at $p$. 
\end{definition}

\begin{definition}[Lines of Curvature]
	If a regular connected curve $C$ in $S$ is such that $\forall p \in C$, the tangent line of $C$ is a principal direction at $p$, then $C$ is said to be a \dfn{line of curvature} of $S$. 
\end{definition}

The following proposition gives us a necessary and sufficient condition for a connected regular curve to be a line of curvature. 

\begin{prop}
	A necessary and sufficient condition for a connected regular curve $C$ on $S$ to be a line of curvature is that: 
	\begin{align*}
		N'(t) = \lambda(t) \alpha'(t)
	\end{align*}
	for any parameterisation $\alpha(t)$ of $C$, where $N(t) = N \circ \alpha(t)$ and $\lambda(t)$ is a differentiable function of $t$. In this case, $-\lambda(t)$ is called the \dfn{principle curvature along} $\alpha'(t)$. 
\end{prop}

This proposition can be used to easily compute the normal curvatures along a given direction in $T_p(S)$. 

\begin{definition}[Gaussian Curvature, Mean Curvature]
	Let $p \in S$ and let d$N_p T_p(S) \rightarrow T_p(S)$ be the differential of the Gauss map. The determinant det$(dN_p)$ is the \dfn{Gaussian Curvature} $\kappa$ of $S$ at $p$. The value $ 1/2 \text{trace}(dN_p)$ is called the \dfn{mean curvature $H$ of $S$ at $p$}. In terms of principal curvatures, these quantities are: 
	\begin{align*}
		& \kappa = k_1 \cdot k_2 \\
		& H = \frac{1}{2} ( k_1 + k_2) 
	\end{align*}
	since $k_1$ and $k_2$ are the eigenvalues. 
\end{definition}

\begin{definition}[Elliptic, Hyperbolic, Parabolic, Planar]
	A point $p \in S$ is called: 
	\begin{itemize}[noitemsep]
		\item \dfn{Elliptic} if det$(dN_p) > 0$ 
		\item \dfn{Hyperbolic} if det$(dN_p) < 0$ 
		\item \dfn{Parabolic} if det$(dN_p) = 0$ and $dN_p \neq 0$
		\item \dfn{Planar} if $dN_p \equiv 0$. 
	\end{itemize}
\end{definition}
Examples of using this classification: 
\begin{itemize}[noitemsep]
	\item Elliptic points: all points on a sphere, the point $(0,0,0)$ of the paraboloid $z = x^2 + ky^2$, $k  > 0$. 
	\item Hyperbolic points: the point $(0,0,0)$ of a hyperbolic paraboloid $z = y^2 - x^2$. 
	\item Parabolic points: the points of a cylinder. 
\end{itemize}

\begin{definition}[Umbilical Points]
	If at $p \in S$, $k_1 = k_2$, then $p$ is called an \dfn{umbilical point} of $S$. The planar points $k_1 = k_2 = 0$ are called umbilical points. The points of a sphere are also umbilical points. 
\end{definition}

\begin{prop}
	If all the points of a connected surface $S$ are umbilical points, then $S$ is either (a) contained in a sphere or (b) contained in a plane. 
\end{prop}

\begin{definition}[Asymptotic Direction or Curve]
	Let $p \in S$. 
	\begin{enumerate}[noitemsep]
		\item An \dfn{asymptotic direction} of $S$ at $p$ is a direction of $T_p(S)$ for which the normal curvature is zero. 
		\item An \dfn{asymptotic curve} of $S$ is a regular connected curve $C \subseteq S$ such that $\forall$ $p \in S$, the tangent line of $C$ at $p$ is an asymptotic direction. 
	\end{enumerate}
\end{definition}

\begin{enumerate}[noitemsep]
	\item At an elliptic point, there are no asymptotic directions. 
	\item The Dupin indicatrix provides a useful geometric interpretation of the asymptotic directions. 
\end{enumerate}

\begin{definition}[Dupin Indicatrix] 
	Let $p \in S$. Then, the \dfn{Dupin Indicatrix} at $p$ is the set of vectors $w$ of $T_p(S)$ such that $II_p(w) = \pm 1$. 
\end{definition}

\begin{definition}[Conjugate Point]
	Let $p \in S$ be a point. Two non-zero vectors $w_1, w_2 \in T_p(S)$ are \dfn{conjugate} if $\langle dN_p(w_1), w_2 \rangle = \langle w_2, dN_p(w_2) \rangle = 0$. Two directions $r_1$, $r_2$ at $p$ are \dfn{conjugate} if a pair of non-zero vectors $w_1$, $w_2$, are parallel to $r_1$, $r_2$, respectively, are conjugate. 
\end{definition}


\subsection{Ruled Surfaces and Minimal Surfaces}


\section{The Intrinsic Geometry of Surfaces}

\subsection{Isometries and Conformal Maps}





\end{document}