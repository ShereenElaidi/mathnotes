\documentclass[11pt]{scrartcl}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{xcolor} 
\usepackage{enumitem}
\newcommand{\R}[0]{\mathbb{R}}
\addtokomafont{section}{\rmfamily\centering\scshape}
% math environments 
\usepackage[utf8]{inputenc}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{ex}{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% definition
\newcommand{\dfn}[1]{\textbf{\underline{#1}}}
\newcommand{\dist}[0]{\mathcal{F}}
\newcommand{\pr}[1]{\mathbb{P}[#1]} 
\newcommand{\stat}[0]{T(X_1, ..., X_n )} 
\newcommand{\C}[0]{\mathbb{C}}
\newcommand{\Z}[0]{\mathbb{Z}}
\newcommand{\Q}[0]{\mathbb{Q}}
\newcommand{\N}[0]{\mathbb{N}}

% Let V be a vs over k...
\newcommand{\vsok}[0]{Let $(V, +, \cdot)$ be a vector space over $K$}

% span 
\newcommand{\vecspan}[1]{\text{span}\{ #1 \}}

% matrix groups
\newcommand{\mat}[1]{\text{MAT}(m \times n, \mathbb{#1})}
\newcommand{\gl}[1]{\text{GL}(n, \mathbb{#1})}

% converge in probability 
\newcommand{\cvp}[0]{\overset{p}{\to}}

% sample mean
\newcommand{\smean}[0]{\frac{1}{n} \sum_{i=1}^n x_i} 

% sample variance
\newcommand{\svar}[0]{\frac{1}{(n-1)} \sum_{i=1}^n (x_i - \overline{x})^2}

% expected value 
\newcommand{\EX}[1]{\mathbb{E}\left[#1 \right]}  
\newcommand{\EXth}[1]{\mathbb{E}_\theta \left[ #1 \right]}

% integral
\newcommand{\idx}[2]{\int_{#1}^{#2}}

% vector
\newcommand{\vect}[1]{\mathbf{#1}}


\title{\textbf{Math 247: Applied Linear Algebra ft. Dual Spaces}}
\author{Shereen Elaidi}
\date{Winter 2018 Term}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}
What is this class about? 
\begin{enumerate}[noitemsep]
	\item Review of linear algebra, but with full proofs. 
	\item More abstract. We will move from $\R^n$ to abstract vector spaces. 
	\item Many objects in maths behave similarly in the sense that they are actually instances of the same phenomenon. So, we want to generalise in order to...
	\begin{enumerate}[noitemsep]
		\item Be more efficient. 
		\item Clarifies the essence of what's behind the object
		\item Groups, rings, fields, and vector spaces are all such objects. 
	\end{enumerate}
\end{enumerate}

\begin{definition}[Group]
	Let $G$ be a set with a binary operation. A \dfn{binary operation} means that you can take two objects in the set, ``add'' them, and return another object in the set. A binary operation $G \times G \rightarrow G$ is a binary operation if it satisfies the following rules: 
	\begin{enumerate}[noitemsep]
		\item \dfn{Law of Associativity}: $\forall a,b,c \in G$: 
		\begin{align*}
			a + (b+c) = (a+b) + c	
		\end{align*}
		$\forall a,b,c \in G$. 
		\item \dfn{Existence of an additive neutral element}: $\exists$ $0 \in G$ such that:
		\begin{align*}
			0 + a = a + 0 = a 
		\end{align*}
		$ \forall a \in G$. 
		\item \dfn{Existence of an additive inverse}: $\forall a \in G$, there exists an element $-a \in G$ such that: 
		\begin{align}
			a + (-a) = (-a) + a = 0 
		\end{align}
	\end{enumerate}
	Any structure with a binary operation satisfying the above rules is called a \dfn{group}. A group always consists of two parts: a set and the operation. For example, a group equipped with addition is denoted $(G, +)$. 
\end{definition}
	Sometimes, we would like to have a group whose binary operation is commutative. Not all groups are commutative! 
\begin{definition}[Abelian Group]
A group $(G, +)$ is called \dfn{abelian} if $\forall a, b \in G$, we have the law of commutativity: 
	\begin{align*}
		a + b = b + a 	
	\end{align*}
\end{definition}

\begin{ex}[Examples of abelian groups] 
	$\R$ with standard addition, $\mathbb{Z}$ with standard addition, $\mathbb{Q}$ with standard addition, $\mathbb{C}$ with standard addition. If we take $n$ copies of $\R$ and $\mathbb{C}$, denoted $\R^n$ and $\mathbb{C}^n$, equipped with component-wise addition, then we also get groups. 
	
	
	
	However, $\mathbb{N}$ with standard addition is not a group simply because there is $\emph{no}$ element with an additive inverse in $\mathbb{N}$ for any value. 
\end{ex}

\begin{ex}[More examples of groups]
	\begin{enumerate}
		\item Matrix groups equipped with matrix addition. The sets of all $m \times n$ matrices with coefficients in $\R$ and $\C$, respectively, are denoted as: 
		\begin{align*}
			& \mat{R} \\
			& \mat{C} 	
		\end{align*}
		when equipped with component-wise addition they form a group. 
	\item Let's look as functions. The set $F(\R)$ of all real-valued with domain $\R$, equipped with addition defined as: 
	\begin{align*}
		(f+g)(x) := f(x) + g(x)	
	\end{align*}
	form a group. These have uncountably many components. The neutral element is the zero function: 
	\begin{align*}
		0(x) := 0 	
	\end{align*}
	and the additive inverse is: 
	\begin{align*}
		(-f)(x) := -f(x)\ \forall x \in \R 	
	\end{align*}
	\end{enumerate}
\end{ex}

\begin{ex}[Non Abelian Group: General Linear]  $\gl{R}$ is called the \dfn{general linear group}. It is the set of all $n \times n$ \emph{invertible} matrices with real coefficients together with the operation of matrix multiplication. The neutral element is the $n \times n$ identity matrix, denoted by $I_n$: 
		\begin{align*}
			I_n A = A I_n = A\ \forall A \in \gl{R}	
		\end{align*}
		Since these matrices, the multiplicative inverse is $A^{-1}$, since $A A^{-1} = I_n$, which is the neutral element. Since associativity is clear, we have that $\gl{R}$ forms a group. 
\end{ex}

\begin{theorem}[Cancellation Law]
	Let $(G, +)$ be a group, and let $a,b,c \in G$ be elements such that: 
	\begin{enumerate}[noitemsep]
		\item $a + b = a + c \Rightarrow b = c$ and 
		\item $b + a = c + a \Rightarrow b = c$
	\end{enumerate}
\end{theorem}

\begin{proof}
	We need to use the properties of a group as discussed. We will first prove (a). To that end, let's first use the existence of an additive inverse:
	\begin{align*} 
		& -a + (a+b ) = -a + (a+c) \\
	 \Rightarrow & (-a + a ) + b = (-a + a) + c & \text{ (Law of Associativity) } \\
	 \Rightarrow &  0 + b = 0 + c &  \text{ (By definition of neutral element) } \\
	 \Rightarrow & b = c &  \text{ (By neutral element law)} 
	\end{align*}
	The proof for (b) is an exercise. 
\end{proof}

\section{Abstract Vector Spaces}
\textbf{Motivation:} generalise $\R^n$. 

\begin{definition}[Vector Space over a field] Let $V$ be a set, and let $K$ be an arbitrary field (in this class we will assume that $K$ is either $\R$ or $\C$) together with two operations: 
\begin{align*}
	& +: V \times V \rightarrow V  \text{ (``Addition'') } \\
	& \cdot: K  \times V \rightarrow V 	 \text{ (``Scalar multiplication'') } 
\end{align*}
and assume that $(V, +)$ is an abelian group and that the $\cdot$ operation obeys the following rules: 
\begin{enumerate}[noitemsep]
	\item ``Associativity'': 
	\begin{align*}
		& (k \ell) v = k ( \ell v)  & \forall k, \ell \in K,  v \in V	
	\end{align*}
	\item ``Distributivity of scalars over vectors'': 
	\begin{align*}
		& ( k + \ell) v = k v + l v\ &  \forall k, \ell \in K, v \in V
	\end{align*}
	\item ``Distributivity of vectors over scalars'': 
	\begin{align*}
		& k (u + v ) = k u + kv\  & \forall k \in K, u, v \in V 	
	\end{align*}
	\item ``The existence of the neutral element of scalar multiplication'' 
	\begin{align*}
		& 1 \cdot v = v 	& \forall v \in V 
	\end{align*}
Then, the tuple $(V, +, \cdot)$ is called a \dfn{vector space over K}. 
\end{enumerate}

\textbf{Remark:} the final axiom is not self-evident. In fact, it is not provable from other axioms. For more details see Assignment 1. 
\end{definition}


\subsection{Many Examples of Vector Spaces}

\begin{ex}[Examples of vector spaces]
	$\R^n$ endowed with standard vector addition and scalar multiplication, denoted $(\R^n, +, \cdot)$ forms a vector space. Additionally, the set of all $n \times m$ matrices with component-wise addition and standard scalar multiplication, denoted by $( \mat{K}, +, \cdot)$ also forms a vector space over $K$. 
\end{ex}

\begin{ex}[Sequence Vector Spaces]
	Let $S$ be the set of all real-valued sequences, i.e.: 
	\begin{align*}
		\{ (a_1, a_2, a_3, ...)\ |\ a_i \in \R \}	
	\end{align*}
	This can be considered a vector space. Addition is component-wise and multiplication is also component-wise. It is similar to $\R^n$, except for $\R^n$ we stop at some finite number $n$. Thus, this space of sequences is \emph{infinite-dimensional}. It is trivially a vector space over $\R$, since checking the axioms amounts to passing them down to the component-level, since all operations are component-wise, and those will trivially follow from the fact that the components are elements of the vector space $(\R, +, \cdot)$. 
\end{ex}

The following examples provide interesting vector spaces for the field of mathematics real analysis: 

\begin{ex}[Spaces of convergent real-valued sequences]
	Let $c$ be the set of all convergent real sequences together with component-wise addition and scalar multiplication. We need to be careful, since something can go wrong. In particular, the $+$ operation needs to be a binary operation, meaning that when you add two members of the set you obtain another member in the set. We will use and not prove the following result from real analysis: the sum of two convergent sequences converge and they converge to the limits of the individual sequences that we add. Thus, the only thing we need to check is that $c$ is closed under $\cdot$ and $+$. However, both of these results are proven in analysis. 
\end{ex}

\begin{ex}[The space of all real-valued sequences that converge to zero $c_0$]
	This space is denoted by $c_0$ and is clearly closed under $+$ and $\cdot$. This forms a subspace of $c$ and is also infinite-dimensional but it is not countable. 
\end{ex}

\begin{ex}[The space $c_{00}$, the space of all real sequences that are eventually zero]. This space contains sequences where all but finitely many terms are different from zero. This is clearly closed under addition and multiplication, and it is a subspace of $c_0$. While this vector space is still infinite dimensional, it is countable. 
\end{ex}

\begin{ex} 
Let $I \subseteq \R$ be an interval. Consider $F(I)$. This is the set of all real-valued functions on $I$ with $+$ and $\cdot $ defined by: 
\begin{align*}
	& (f+g)(x) = f(x) + g(x) \\
	& (kf) (x) = kf(x)	
\end{align*}
	This example will be investigated in Assignment 1. It is a vector space. 
\end{ex}

There are a lot of functions that mathematicians care about: differentiable functions, continuous functions, those that have Taylor expansions on $I$, etc. These are all vector spaces. 

\begin{ex} 
(a) Let $F^c(I)$ be the set of all continuous functions on $I$. 	(b) Let $C^0(I)$ be the set of all continuously differentiable functions on $I$. (c) Let $C^n(I)$ be the set of all functions on $I$ with continuous derivatives up to order $n$. (d) Let $C^\infty (I)$ be all functions on $I$ with derivatives of all order. Define operations on (a) - (d) as was defined in the previous example. These are all vector spaces over $\R$. 
\end{ex}


Linear algebra is also useful for solving differential equations. For example, consider the set of all solutions of $y'' + y = 0$, or more generally, the set of all solution to any linear differential equation: 
\begin{align*}
	a_n y^{(n)} + a_{(n-1)}y^{(n-1)} + ... + a_ny' + a_0 y = 0 	
\end{align*}
where $a_0, ..., a_n \in \R$. This is an important set of functions. We really care about the closedness of the so-called solution space. So, we need to verify that solutions are closed under $+$ and $\cdot$. To that end, let $y_1, y_2$ be solutions of $y'' + y = 0$. Then we have: 
\begin{align*}
	& y_1'' + y_1 = 0 \text{ and } y_2'' + y_2 ' = 0 	
\end{align*}
Now let's study $(y_1 + y_2)'' + (y_1 + y_2)$. By the linearity of differentiation: 
\begin{align*}
	y_1 '' + y_2 '' + y_1 + y_2 = (y_1'' + y_1) + (y_2'' + y_2) = 0 + 0 = 0 	
\end{align*}
Hence, we have closure under addition. For scalars: 
\begin{align*}
	(ky_1)'' + ky_1 = ky_1'' +ky_1 + k(y_1'' + y_1) = 0 	
\end{align*}
which implies that $ky_1$ is also a solution, and we therefore have closure under scalar multiplication. 

Once we determine that the solutions are a vector space, then all we need to do is obtain a basis and take all \emph{linear combinations} to obtain more solutions of DEs. 

\subsection{Consequences of the Axioms}
This section has some immediate consequences from the axioms of a vector space. 

\begin{theorem}
	Let $(V, +, \cdot)$ be a vector space over $K$. Then: 
	\begin{enumerate}[noitemsep]
		\item $k \cdot 0 = 0$ $\forall$ $k \in K$. 
		\item $0 \cdot v = 0$ $\forall$ $v \in V$. 
		\item $k \cdot v = 0$ $\iff$ $k =0 $ or $v=0$. 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item We need to show that $k \cdot 0 =0$. This is a ``fun trick proof.'' 
		\begin{align*}
			& k \cdot 0 = k \cdot (0 + 0) & \text{ ($0$ is the neutral element of $+$ ) } \\
			& k \cdot 0 = k \cdot 0 + k \cdot 0  & \text{ (distributivity of scalars over vectors) } \\
			& 0 + k \cdot 0 = k \cdot 0 + k \cdot 0  & \text{ (by the neutral element law) } \\
			& 0  = k \cdot 0 & \text{ (by the cancellation law) } 
		\end{align*}
		\item 
		\begin{align*}
			 & 0 \cdot v = (0 + 0) v & \text{ ($0$ is neutral element of addition in $K$)} \\
			 & 0 \cdot v = 0 v + 0 v & \text{ (distributivity of scalars over vectors)} \\
			 & 0 + 0 \cdot v = 0 \cdot v + 0 \cdot v & \text{ (neutral element law) } \\
			 & 0 = 0 \cdot v & \text{ (cancellation law)}
		\end{align*}
		\item There are two directions to show here.
		\newline 
	``$\Leftarrow $'': if $k$ = 0 or $v =0$, then it follows from (a) + (b), respectively, that $kv =0$.   \\
	``$\Rightarrow$'': there are two cases to consider. If $k=0$, then we are done. If $k \neq 0$, then we need to prove that $v=0$. Take $kv =0$. It has a multiplicative inverse: 
		\begin{align*}
			kv = 0\ \Rightarrow \left( \frac{1}{k} \right) (kv) = \frac{1}{k} 0 = 0 
		\end{align*}
		Apply associativity to the $\left( \frac{1}{k} \right) (kv)$ term: 
		\begin{align*}
			\left( \frac{1}{k} k \right) v \underbrace{=}_{\text{axiom}} 1v = v  
		\end{align*}
		This implies that $v=0$, and so we are done. 
	\end{enumerate}
\end{proof}

\begin{theorem}
	Let $(V, +, \cdot)$ be a vector space over $K$. Then: 
	\begin{enumerate}[noitemsep]
		\item $n \cdot v = \underbrace{v + ... + v}_{\text{$n$ times}}$ for all $n \in \mathbb{N}$. 
		\item $0 \cdot v =0$. 
		\item $(-1)v = -v$ (the additive inverse) for all $v \in V$. 
	\end{enumerate}
\end{theorem}


\begin{proof}
	\begin{enumerate}[noitemsep]
		\item We will prove this one by induction. The base case is obvious since it is an axiom of a vector space ($1v = v$). Now for the inductive step assume that 
		\begin{align*}
			nv = \underbrace{v + ... + v}_{\text{ $n$ times}}
		\end{align*}
		for some $n$ in $\mathbb{N}$. Applying distributivity: 
		\begin{align*}
			(n+1) v = nv + 1v = \underbrace{v + ... + v}_{n \text{ times, by IH}} + \underbrace{v}_{\text{by axiom}} = \underbrace{v + ... + v}_{n+1 \text{ times}} 
		\end{align*}
		where the final equality follows from the fact that we are in an abelian group. 
		\item We get this from (1); we have $v$, zero times. 
		\item We want to prove that $(-1)v$ is the additive inverse. We can simply add this candidate and prove that the sum is zero: 
		\begin{align*}
			& v + (-1)v & \text{ (use the axiom) } \\
			& (1)v + (-1)v & \text{ (use the other axiom) } \\
			& (1 + (-1)) v & \text{ (distributivity) } \\
			& 0 v 
		\end{align*}
		and we know that $0v =0$ by (2). We thus have that $(-1)v$ is the additive inverse of $v$, which is what we wanted to show. 
	\end{enumerate}
\end{proof}

\textbf{Exercise:} prove that $(-n)v = \underbrace{(-v) + ... + (-v)}_{n \text{ times}}$. 


\subsection{Subspaces}

\begin{definition}[Subspaces]
	Let $(V, +, \cdot)$ be a vector space over $K$. Let $W \subseteq V$. Then, we say that $W$ is a \dfn{subspace} of $V$, in symbols $W \leq V$, if $(W, +, \cdot)$ is a vector space over $K$ in its own right. 
\end{definition}

\textbf{\underline{Q}}: How can we determine if something is a subspace? This question motivates the next theorem. 

\begin{theorem}
	Let $(V, +, \cdot)$ be a vector space over $K$, $W \subseteq V$. Then, $W \leq V$ if and only if: 
	\begin{enumerate}[noitemsep]
		\item $W$ is closed under addition and multiplication. 
		\item $0 \in W$.
	\end{enumerate}
\end{theorem}

Note that the final requirement in the theorem is actually a proxy for requiring that $W$ is not the empty set. 

\begin{proof}
	``$\Rightarrow$'': Since $W$ is a vector space over $K$, $W$ is closed under $+$ and $\cdot$. Furthermore, $0 \in W$ since $(W, +)$ is an abelian group and thus contains the neutral element. 
	\newline
	\newline
	``$\Leftarrow$'': This is a straightforward verification of the axioms of a vector space. Most of the axioms will simply follow, since as a subset of $V$ $W$ will inherit a lot of rules. 
	\begin{itemize}[noitemsep]
		\item $W$ will inherit associativity from $V$. 
		\item $W$ will inherit commutativity from $V$. 
		\item $W$ contains the neutral element by condition. 
		\item Let $u$ be an arbitrary vector. Since $U$ is closed under scalar multiplication, we have that $(-1)u = -u \in U$. This proves the existence of an additive inverse. 
		\item All of the scalar multiplication axioms will hold since they hold $\forall$ $v \in V$. 
	\end{itemize}
	This implies that $W$ is a vector space with respect to $+$ and $\cdot$, which gives us that $W \leq V$, which is what we wanted to show. 
\end{proof}
Here are some examples of subspaces of vector spaces. 
\begin{ex}
	The following are the \emph{only} subspaces of $\R^2$: 
	\begin{enumerate}[noitemsep]
		\item $\{ 0 \}$
		\item All lines through the origin. 
		\item The whole of $\R^2$. 
	\end{enumerate}
\end{ex}

\begin{ex}
Let $V:= \mat{K}$. Let $U:=$ the set of all \dfn{upper triangular} matrices i.e.: 
\begin{align*}
	U := \{\ A = (a_{ij}) \in \mat{K}\ |\ a_{ij} = 0\ \forall i > k,\ 1 \leq i,j \leq n \} 
\end{align*}	
This is a subspace. 
\end{ex}

\begin{ex} 
	Recall sequence spaces. We have the following chain of subspaces: $C_{00} \leq C_0 \leq C \leq S$, where $S$ denotes a sequence space.	
\end{ex}

\begin{ex} 
	Let $P_n(K)$ be the set of polynomials of degree at most $n$. Let $P(K)$ be the set of all polynomials, and equip it with standard addition of polynomials. Then: 
	\begin{align*}
			P_0(K) \leq P_1(K) \leq P_2(K) \leq \cdots \leq P_n(K) \leq \cdots \leq P 
	\end{align*}	
	The dimension of each $P_n$ is $n+1$. 
\end{ex}

\begin{theorem} 
Let $(V, +, \cdot)$ be a vector space over $K$. 
	\begin{enumerate}
		\item Let $U$ be a subspace of $V$ and let $W$ be a vector space. Then: $U \cap W \leq V$. 
		\item Let $U_1, ..., U_n \leq V$. Then: $U_1 \cap \cdots \cap U_n = \bigcap_{i=1}^n U_i \leq V$. 
		\item Let $I$ be an arbitrary index set, and let $U_i \leq V$ $\forall i \in I$. Then: $\bigcap_{i \in I} V_I \leq V$. 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item Another straightforward verification of the axioms. 
		\begin{enumerate}[noitemsep]
			\item $0 \in U$, and $0 \in W$ since $W$ is a vector space. Thus $0 \in W \cap U$. 
			\item Now we check closure under addition and scalar multiplication.  
			\begin{enumerate}[noitemsep]
				\item Let $v_1, v_2 \in U \cap W$. We thus especially have that $v_1 \in U$ and $v_2 \in U$, and so $v_1 + v_2 \in U$ since $U$ is closed under addition. Similarly, $v_1 + v_2 \in W$. Since the sum is in both sets, it is in the intersection of the sets. Hence, it is closed under addition. 
				\item Since $u_1 \in U$, $K u_1 \in V$ for all $k \in K$, since $U$ is a vector space. Similarly, if $u_2 \in W$, then $k u_2 \in W$ for all $k \in K$. Thus, the product is in the intersection. 
			\end{enumerate}
		\end{enumerate}
		So, by the subspace criteria, $U \cap W \leq V$. 
		\item Exercise. 
		\item Exercise. 
	\end{enumerate}
\end{proof}

So, intersections are generally not a problem. However, unions are a more difficult case. For example let $V = \R^2$, and define $U$ to be the $x$-axis and $W$ to be the $y$-axis. Consider $U \cup W$. It is not a line or plane, so it is not a subspace. While this union of subspaces will always contain zero and while it will always be closed under scalar multiplications, it is \underline{not closed under addition}. Unions in general are not subspaces, which motivates the following theorem. The proof is in assignment two. 

\begin{theorem}
	\vsok. Then, $U \cup V \leq V$ if and only if $U \subseteq W$ or $W \subseteq U$ (i.e., the trivial cases). 
\end{theorem} 
\begin{proof}
	Homework. 
\end{proof}

\begin{definition}[Sum of Vector Spaces]
	\vsok. Let $U, W \leq V$. Then: 
	\begin{align*}
		U + W := \{ w + w\ |\ u \in U,\ w \in W \} 
	\end{align*}
	which is called the \dfn{sum} of $U$ and $W$, is defined as follows: all the possible sums with one vector from $U$ and one vector from $W$. 
\end{definition}

\textbf{\underline{Q}}: Why do we bother defining the sum? 
\begin{enumerate}[noitemsep]
	\item It is a subspace of $V$. 
	\item It is the \emph{smallest} subspace of $V$ that contains the union of $U$ and $W$ (``generator''). 
\end{enumerate}

\begin{theorem}
	\vsok. Let $U, W \leq V$. Then: 
	\begin{enumerate}[noitemsep]
		\item $U + W \leq V$. 
		\item $U+W$ is the \emph{smallest} subspace of $V$ that contains \underline{both} $U$ and $V$ and thus their \underline{union}. This means that if $\widetilde{V}$ is any subspace of $V$ with $U \cup W \subseteq \widetilde{V}$, then the sum $U + W \subseteq \widetilde{V}$. 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item Another straightforward verification of the axioms. 
		\begin{enumerate}[noitemsep]
			\item We have that $0 \in W$ and $0 \in W$, and so $0 = 0 + 0 \in U +W$. 
			\item Let $v_1, v_2 \in U + W$. Then, $\exists$ $u_1, u_2 \in U$, $w_1, w_2 \in W$, such that: 
			\begin{align*}
				& v_1 = u_1 + w_1 \\
				& v_2 = u_2 + w_2 
			\end{align*}
			Adding these together we obtain: 
			\begin{align*}
					v_1 + v_2 = \underbrace{(u_1 + u_2)}_{\in U} + \underbrace{(w_1 + w_2)}_{\in W} \in U + W 
			\end{align*}
			\item Now for scalar multiplication. Write 
			\begin{align*}
				kv_1 & = k(u_1 + w_1) & \\
					& = k u_1 + k w_1 & \text{ (distributivity) } 
			\end{align*}
			$ku_1 \in U$ and $k w_1 \in W$, which means that $kv_1 \in U+W$, which is what we wanted to show. 
			Therefore, $U+W \leq V$.
		\end{enumerate}
		\item Let $\widetilde{V}$ be any subspace of $V$ that contains both $U$ and $W$. Let $u \in U$ and $w \in W$ be arbitrary. THen, by definition, $u+w \in U+W$. Every element bust thus be in $\widetilde{V}$, which implies that $U+W$ must be contained in $\widetilde{V}$, which proves that $U+W$ is the smallest subspace of $V$ containing $U \cup W$. 
	\end{enumerate}
\end{proof}

In a similar fashion we can define $U_1 + ... + U_n$, where $U_1, ..., U_n$ are all subspaces of $V$ as: 
\begin{align*}
		\{ u_1 + ... + u_n\ |\ u_k \in U_k\ \forall\ 1 \leq k \leq n \} 
\end{align*}
Then it's easy to verify that $U_1 + ... + U_n \leq V$. 

\subsection{Linear Combination and Span}

\begin{definition}[Linear Combination]
	\vsok. Let $v_1, ..., v_n \in V$ and let $k_1, ..., k_n \in K$. Then, an expression of the form
	\begin{align}
		k_1 v_1 + ... + k_n v_n 	
	\end{align}	
	is called a \dfn{linear combination} of $v_1, ..., v_n$. 
\end{definition}
Note that it is possible to extend this to infinitely many vectors: 
\begin{align*}
	k_1 v_1 + k_2 v_2 + ...
\end{align*}
but this is \emph{completely meaningless}. This is a bad thing since we do not know what it means to add infinitely many vectors. We have not developed a notion of convergence or distance on a vector space in this class, so we will not discuss this. What we an do is write $k_1 v_1 + k_2 v_2 + ...$ where all but \emph{finitely many} of the $k_i =0$, but this can be viewed as an abuse of notation, since the sum is actually finite. 

\begin{definition}[Span]
	\vsok. Let $v_1, ..., v_n \in V$. Then: 
	\begin{align}
		\vecspan{v_1, ...v_n} := \{ k_1 v_1 + ... + k_n v_n\ |\ 1 \leq i \leq n\ k_i \in K \} 
	\end{align}
	is called the \dfn{span} of $v_1, ..., v_n$. It is the set of all possible linear combinations. We can extend this to infinitely many vectors $v_1, v_2, ... \in V$ as: 
	\begin{align*}
		\vecspan{v_1, ..., v_n } := \{ k_1 v_1 + k_2 v_2 + ...\ |\ k_i \in K, \text{ all but finitely many $k_i$ are zero } \} 	
	\end{align*}

\end{definition}

\end{document}