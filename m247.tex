\documentclass[11pt]{scrartcl}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{xcolor} 
\usepackage{enumitem}
\newcommand{\R}[0]{\mathbb{R}}
\addtokomafont{section}{\rmfamily\centering\scshape}
% math environments 
\usepackage[utf8]{inputenc}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{ex}{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}



% definition
\newcommand{\dfn}[1]{\textbf{\underline{#1}}}
\newcommand{\dist}[0]{\mathcal{F}}
\newcommand{\pr}[1]{\mathbb{P}[#1]} 
\newcommand{\stat}[0]{T(X_1, ..., X_n )} 
\newcommand{\C}[0]{\mathbb{C}}
\newcommand{\Z}[0]{\mathbb{Z}}
\newcommand{\Q}[0]{\mathbb{Q}}
\newcommand{\N}[0]{\mathbb{N}}
\newcommand{\kernel}[1]{\text{ker}[#1]}
\newcommand{\image}[1]{\text{im}[#1]}


% Let V be a vs over k...
\newcommand{\vsok}[0]{Let $(V, +, \cdot)$ be a vector space over $K$}

% span 
\newcommand{\vecspan}[1]{\text{span}\{ #1 \}}

% matrix groups
\newcommand{\mat}[1]{\text{MAT}(m \times n, \mathbb{#1})}
\newcommand{\gl}[1]{\text{GL}(n, \mathbb{#1})}
\newcommand{\matn}[1]{\text{MAT}(n\times n, \mathbb{#1})}

% converge in probability 
\newcommand{\cvp}[0]{\overset{p}{\to}}

% sample mean
\newcommand{\smean}[0]{\frac{1}{n} \sum_{i=1}^n x_i} 

% sample variance
\newcommand{\svar}[0]{\frac{1}{(n-1)} \sum_{i=1}^n (x_i - \overline{x})^2}

% expected value 
\newcommand{\EX}[1]{\mathbb{E}\left[#1 \right]}  
\newcommand{\EXth}[1]{\mathbb{E}_\theta \left[ #1 \right]}

% integral
\newcommand{\idx}[2]{\int_{#1}^{#2}}

% vector
\newcommand{\vect}[1]{\mathbf{#1}}


\title{\textbf{Math 247: Applied Linear Algebra ft. Dual Spaces}}
\author{Shereen Elaidi}
\date{Winter 2018 Term}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}
What is this class about? 
\begin{enumerate}[noitemsep]
	\item Review of linear algebra, but with full proofs. 
	\item More abstract. We will move from $\R^n$ to abstract vector spaces. 
	\item Many objects in maths behave similarly in the sense that they are actually instances of the same phenomenon. So, we want to generalise in order to...
	\begin{enumerate}[noitemsep]
		\item Be more efficient. 
		\item Clarifies the essence of what's behind the object
		\item Groups, rings, fields, and vector spaces are all such objects. 
	\end{enumerate}
\end{enumerate}

\begin{definition}[Group]
	Let $G$ be a set with a binary operation. A \dfn{binary operation} means that you can take two objects in the set, ``add'' them, and return another object in the set. A binary operation $G \times G \rightarrow G$ is a binary operation if it satisfies the following rules: 
	\begin{enumerate}[noitemsep]
		\item \dfn{Law of Associativity}: $\forall a,b,c \in G$: 
		\begin{align*}
			a + (b+c) = (a+b) + c	
		\end{align*}
		$\forall a,b,c \in G$. 
		\item \dfn{Existence of an additive neutral element}: $\exists$ $0 \in G$ such that:
		\begin{align*}
			0 + a = a + 0 = a 
		\end{align*}
		$ \forall a \in G$. 
		\item \dfn{Existence of an additive inverse}: $\forall a \in G$, there exists an element $-a \in G$ such that: 
		\begin{align}
			a + (-a) = (-a) + a = 0 
		\end{align}
	\end{enumerate}
	Any structure with a binary operation satisfying the above rules is called a \dfn{group}. A group always consists of two parts: a set and the operation. For example, a group equipped with addition is denoted $(G, +)$. 
\end{definition}
	Sometimes, we would like to have a group whose binary operation is commutative. Not all groups are commutative! 
\begin{definition}[Abelian Group]
A group $(G, +)$ is called \dfn{abelian} if $\forall a, b \in G$, we have the law of commutativity: 
	\begin{align*}
		a + b = b + a 	
	\end{align*}
\end{definition}

\begin{ex}[Examples of abelian groups] 
	$\R$ with standard addition, $\mathbb{Z}$ with standard addition, $\mathbb{Q}$ with standard addition, $\mathbb{C}$ with standard addition. If we take $n$ copies of $\R$ and $\mathbb{C}$, denoted $\R^n$ and $\mathbb{C}^n$, equipped with component-wise addition, then we also get groups. 
	
	
	
	However, $\mathbb{N}$ with standard addition is not a group simply because there is $\emph{no}$ element with an additive inverse in $\mathbb{N}$ for any value. 
\end{ex}

\begin{ex}[More examples of groups]
	\begin{enumerate}
		\item Matrix groups equipped with matrix addition. The sets of all $m \times n$ matrices with coefficients in $\R$ and $\C$, respectively, are denoted as: 
		\begin{align*}
			& \mat{R} \\
			& \mat{C} 	
		\end{align*}
		when equipped with component-wise addition they form a group. 
	\item Let's look as functions. The set $F(\R)$ of all real-valued with domain $\R$, equipped with addition defined as: 
	\begin{align*}
		(f+g)(x) := f(x) + g(x)	
	\end{align*}
	form a group. These have uncountably many components. The neutral element is the zero function: 
	\begin{align*}
		0(x) := 0 	
	\end{align*}
	and the additive inverse is: 
	\begin{align*}
		(-f)(x) := -f(x)\ \forall x \in \R 	
	\end{align*}
	\end{enumerate}
\end{ex}

\begin{ex}[Non Abelian Group: General Linear]  $\gl{R}$ is called the \dfn{general linear group}. It is the set of all $n \times n$ \emph{invertible} matrices with real coefficients together with the operation of matrix multiplication. The neutral element is the $n \times n$ identity matrix, denoted by $I_n$: 
		\begin{align*}
			I_n A = A I_n = A\ \forall A \in \gl{R}	
		\end{align*}
		Since these matrices, the multiplicative inverse is $A^{-1}$, since $A A^{-1} = I_n$, which is the neutral element. Since associativity is clear, we have that $\gl{R}$ forms a group. 
\end{ex}

\begin{theorem}[Cancellation Law]
	Let $(G, +)$ be a group, and let $a,b,c \in G$ be elements such that: 
	\begin{enumerate}[noitemsep]
		\item $a + b = a + c \Rightarrow b = c$ and 
		\item $b + a = c + a \Rightarrow b = c$
	\end{enumerate}
\end{theorem}

\begin{proof}
	We need to use the properties of a group as discussed. We will first prove (a). To that end, let's first use the existence of an additive inverse:
	\begin{align*} 
		& -a + (a+b ) = -a + (a+c) \\
	 \Rightarrow & (-a + a ) + b = (-a + a) + c & \text{ (Law of Associativity) } \\
	 \Rightarrow &  0 + b = 0 + c &  \text{ (By definition of neutral element) } \\
	 \Rightarrow & b = c &  \text{ (By neutral element law)} 
	\end{align*}
	The proof for (b) is an exercise. 
\end{proof}

\section{Abstract Vector Spaces}
\textbf{Motivation:} generalise $\R^n$. 

\begin{definition}[Vector Space over a field] Let $V$ be a set, and let $K$ be an arbitrary field (in this class we will assume that $K$ is either $\R$ or $\C$) together with two operations: 
\begin{align*}
	& +: V \times V \rightarrow V  \text{ (``Addition'') } \\
	& \cdot: K  \times V \rightarrow V 	 \text{ (``Scalar multiplication'') } 
\end{align*}
and assume that $(V, +)$ is an abelian group and that the $\cdot$ operation obeys the following rules: 
\begin{enumerate}[noitemsep]
	\item ``Associativity'': 
	\begin{align*}
		& (k \ell) v = k ( \ell v)  & \forall k, \ell \in K,  v \in V	
	\end{align*}
	\item ``Distributivity of scalars over vectors'': 
	\begin{align*}
		& ( k + \ell) v = k v + l v\ &  \forall k, \ell \in K, v \in V
	\end{align*}
	\item ``Distributivity of vectors over scalars'': 
	\begin{align*}
		& k (u + v ) = k u + kv\  & \forall k \in K, u, v \in V 	
	\end{align*}
	\item ``The existence of the neutral element of scalar multiplication'' 
	\begin{align*}
		& 1 \cdot v = v 	& \forall v \in V 
	\end{align*}
Then, the tuple $(V, +, \cdot)$ is called a \dfn{vector space over K}. 
\end{enumerate}

\textbf{Remark:} the final axiom is not self-evident. In fact, it is not provable from other axioms. For more details see Assignment 1. 
\end{definition}


\subsection{Many Examples of Vector Spaces}

\begin{ex}[Examples of vector spaces]
	$\R^n$ endowed with standard vector addition and scalar multiplication, denoted $(\R^n, +, \cdot)$ forms a vector space. Additionally, the set of all $n \times m$ matrices with component-wise addition and standard scalar multiplication, denoted by $( \mat{K}, +, \cdot)$ also forms a vector space over $K$. 
\end{ex}

\begin{ex}[Sequence Vector Spaces]
	Let $S$ be the set of all real-valued sequences, i.e.: 
	\begin{align*}
		\{ (a_1, a_2, a_3, ...)\ |\ a_i \in \R \}	
	\end{align*}
	This can be considered a vector space. Addition is component-wise and multiplication is also component-wise. It is similar to $\R^n$, except for $\R^n$ we stop at some finite number $n$. Thus, this space of sequences is \emph{infinite-dimensional}. It is trivially a vector space over $\R$, since checking the axioms amounts to passing them down to the component-level, since all operations are component-wise, and those will trivially follow from the fact that the components are elements of the vector space $(\R, +, \cdot)$. 
\end{ex}

The following examples provide interesting vector spaces for the field of mathematics real analysis: 

\begin{ex}[Spaces of convergent real-valued sequences]
	Let $c$ be the set of all convergent real sequences together with component-wise addition and scalar multiplication. We need to be careful, since something can go wrong. In particular, the $+$ operation needs to be a binary operation, meaning that when you add two members of the set you obtain another member in the set. We will use and not prove the following result from real analysis: the sum of two convergent sequences converge and they converge to the limits of the individual sequences that we add. Thus, the only thing we need to check is that $c$ is closed under $\cdot$ and $+$. However, both of these results are proven in analysis. 
\end{ex}

\begin{ex}[The space of all real-valued sequences that converge to zero $c_0$]
	This space is denoted by $c_0$ and is clearly closed under $+$ and $\cdot$. This forms a subspace of $c$ and is also infinite-dimensional but it is not countable. 
\end{ex}

\begin{ex}[The space $c_{00}$, the space of all real sequences that are eventually zero]. This space contains sequences where all but finitely many terms are different from zero. This is clearly closed under addition and multiplication, and it is a subspace of $c_0$. While this vector space is still infinite dimensional, it is countable. 
\end{ex}

\begin{ex} 
Let $I \subseteq \R$ be an interval. Consider $F(I)$. This is the set of all real-valued functions on $I$ with $+$ and $\cdot $ defined by: 
\begin{align*}
	& (f+g)(x) = f(x) + g(x) \\
	& (kf) (x) = kf(x)	
\end{align*}
	This example will be investigated in Assignment 1. It is a vector space. 
\end{ex}

There are a lot of functions that mathematicians care about: differentiable functions, continuous functions, those that have Taylor expansions on $I$, etc. These are all vector spaces. 

\begin{ex} 
(a) Let $F^c(I)$ be the set of all continuous functions on $I$. 	(b) Let $C^0(I)$ be the set of all continuously differentiable functions on $I$. (c) Let $C^n(I)$ be the set of all functions on $I$ with continuous derivatives up to order $n$. (d) Let $C^\infty (I)$ be all functions on $I$ with derivatives of all order. Define operations on (a) - (d) as was defined in the previous example. These are all vector spaces over $\R$. 
\end{ex}


Linear algebra is also useful for solving differential equations. For example, consider the set of all solutions of $y'' + y = 0$, or more generally, the set of all solution to any linear differential equation: 
\begin{align*}
	a_n y^{(n)} + a_{(n-1)}y^{(n-1)} + ... + a_ny' + a_0 y = 0 	
\end{align*}
where $a_0, ..., a_n \in \R$. This is an important set of functions. We really care about the closedness of the so-called solution space. So, we need to verify that solutions are closed under $+$ and $\cdot$. To that end, let $y_1, y_2$ be solutions of $y'' + y = 0$. Then we have: 
\begin{align*}
	& y_1'' + y_1 = 0 \text{ and } y_2'' + y_2 ' = 0 	
\end{align*}
Now let's study $(y_1 + y_2)'' + (y_1 + y_2)$. By the linearity of differentiation: 
\begin{align*}
	y_1 '' + y_2 '' + y_1 + y_2 = (y_1'' + y_1) + (y_2'' + y_2) = 0 + 0 = 0 	
\end{align*}
Hence, we have closure under addition. For scalars: 
\begin{align*}
	(ky_1)'' + ky_1 = ky_1'' +ky_1 + k(y_1'' + y_1) = 0 	
\end{align*}
which implies that $ky_1$ is also a solution, and we therefore have closure under scalar multiplication. 

Once we determine that the solutions are a vector space, then all we need to do is obtain a basis and take all \emph{linear combinations} to obtain more solutions of DEs. 

\subsection{Consequences of the Axioms}
This section has some immediate consequences from the axioms of a vector space. 

\begin{theorem}
	Let $(V, +, \cdot)$ be a vector space over $K$. Then: 
	\begin{enumerate}[noitemsep]
		\item $k \cdot 0 = 0$ $\forall$ $k \in K$. 
		\item $0 \cdot v = 0$ $\forall$ $v \in V$. 
		\item $k \cdot v = 0$ $\iff$ $k =0 $ or $v=0$. 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item We need to show that $k \cdot 0 =0$. This is a ``fun trick proof.'' 
		\begin{align*}
			& k \cdot 0 = k \cdot (0 + 0) & \text{ ($0$ is the neutral element of $+$ ) } \\
			& k \cdot 0 = k \cdot 0 + k \cdot 0  & \text{ (distributivity of scalars over vectors) } \\
			& 0 + k \cdot 0 = k \cdot 0 + k \cdot 0  & \text{ (by the neutral element law) } \\
			& 0  = k \cdot 0 & \text{ (by the cancellation law) } 
		\end{align*}
		\item 
		\begin{align*}
			 & 0 \cdot v = (0 + 0) v & \text{ ($0$ is neutral element of addition in $K$)} \\
			 & 0 \cdot v = 0 v + 0 v & \text{ (distributivity of scalars over vectors)} \\
			 & 0 + 0 \cdot v = 0 \cdot v + 0 \cdot v & \text{ (neutral element law) } \\
			 & 0 = 0 \cdot v & \text{ (cancellation law)}
		\end{align*}
		\item There are two directions to show here.
		\newline 
	``$\Leftarrow $'': if $k$ = 0 or $v =0$, then it follows from (a) + (b), respectively, that $kv =0$.   \\
	``$\Rightarrow$'': there are two cases to consider. If $k=0$, then we are done. If $k \neq 0$, then we need to prove that $v=0$. Take $kv =0$. It has a multiplicative inverse: 
		\begin{align*}
			kv = 0\ \Rightarrow \left( \frac{1}{k} \right) (kv) = \frac{1}{k} 0 = 0 
		\end{align*}
		Apply associativity to the $\left( \frac{1}{k} \right) (kv)$ term: 
		\begin{align*}
			\left( \frac{1}{k} k \right) v \underbrace{=}_{\text{axiom}} 1v = v  
		\end{align*}
		This implies that $v=0$, and so we are done. 
	\end{enumerate}
\end{proof}

\begin{theorem}
	Let $(V, +, \cdot)$ be a vector space over $K$. Then: 
	\begin{enumerate}[noitemsep]
		\item $n \cdot v = \underbrace{v + ... + v}_{\text{$n$ times}}$ for all $n \in \mathbb{N}$. 
		\item $0 \cdot v =0$. 
		\item $(-1)v = -v$ (the additive inverse) for all $v \in V$. 
	\end{enumerate}
\end{theorem}


\begin{proof}
	\begin{enumerate}[noitemsep]
		\item We will prove this one by induction. The base case is obvious since it is an axiom of a vector space ($1v = v$). Now for the inductive step assume that 
		\begin{align*}
			nv = \underbrace{v + ... + v}_{\text{ $n$ times}}
		\end{align*}
		for some $n$ in $\mathbb{N}$. Applying distributivity: 
		\begin{align*}
			(n+1) v = nv + 1v = \underbrace{v + ... + v}_{n \text{ times, by IH}} + \underbrace{v}_{\text{by axiom}} = \underbrace{v + ... + v}_{n+1 \text{ times}} 
		\end{align*}
		where the final equality follows from the fact that we are in an abelian group. 
		\item We get this from (1); we have $v$, zero times. 
		\item We want to prove that $(-1)v$ is the additive inverse. We can simply add this candidate and prove that the sum is zero: 
		\begin{align*}
			& v + (-1)v & \text{ (use the axiom) } \\
			& (1)v + (-1)v & \text{ (use the other axiom) } \\
			& (1 + (-1)) v & \text{ (distributivity) } \\
			& 0 v 
		\end{align*}
		and we know that $0v =0$ by (2). We thus have that $(-1)v$ is the additive inverse of $v$, which is what we wanted to show. 
	\end{enumerate}
\end{proof}

\textbf{Exercise:} prove that $(-n)v = \underbrace{(-v) + ... + (-v)}_{n \text{ times}}$. 


\subsection{Subspaces}

\begin{definition}[Subspaces]
	Let $(V, +, \cdot)$ be a vector space over $K$. Let $W \subseteq V$. Then, we say that $W$ is a \dfn{subspace} of $V$, in symbols $W \leq V$, if $(W, +, \cdot)$ is a vector space over $K$ in its own right. 
\end{definition}

\textbf{\underline{Q}}: How can we determine if something is a subspace? This question motivates the next theorem. 

\begin{theorem}
	Let $(V, +, \cdot)$ be a vector space over $K$, $W \subseteq V$. Then, $W \leq V$ if and only if: 
	\begin{enumerate}[noitemsep]
		\item $W$ is closed under addition and multiplication. 
		\item $0 \in W$.
	\end{enumerate}
\end{theorem}

Note that the final requirement in the theorem is actually a proxy for requiring that $W$ is not the empty set. 

\begin{proof}
	``$\Rightarrow$'': Since $W$ is a vector space over $K$, $W$ is closed under $+$ and $\cdot$. Furthermore, $0 \in W$ since $(W, +)$ is an abelian group and thus contains the neutral element. 
	\newline
	\newline
	``$\Leftarrow$'': This is a straightforward verification of the axioms of a vector space. Most of the axioms will simply follow, since as a subset of $V$ $W$ will inherit a lot of rules. 
	\begin{itemize}[noitemsep]
		\item $W$ will inherit associativity from $V$. 
		\item $W$ will inherit commutativity from $V$. 
		\item $W$ contains the neutral element by condition. 
		\item Let $u$ be an arbitrary vector. Since $U$ is closed under scalar multiplication, we have that $(-1)u = -u \in U$. This proves the existence of an additive inverse. 
		\item All of the scalar multiplication axioms will hold since they hold $\forall$ $v \in V$. 
	\end{itemize}
	This implies that $W$ is a vector space with respect to $+$ and $\cdot$, which gives us that $W \leq V$, which is what we wanted to show. 
\end{proof}
Here are some examples of subspaces of vector spaces. 
\begin{ex}
	The following are the \emph{only} subspaces of $\R^2$: 
	\begin{enumerate}[noitemsep]
		\item $\{ 0 \}$
		\item All lines through the origin. 
		\item The whole of $\R^2$. 
	\end{enumerate}
\end{ex}

\begin{ex}
Let $V:= \mat{K}$. Let $U:=$ the set of all \dfn{upper triangular} matrices i.e.: 
\begin{align*}
	U := \{\ A = (a_{ij}) \in \mat{K}\ |\ a_{ij} = 0\ \forall i > k,\ 1 \leq i,j \leq n \} 
\end{align*}	
This is a subspace. 
\end{ex}

\begin{ex} 
	Recall sequence spaces. We have the following chain of subspaces: $C_{00} \leq C_0 \leq C \leq S$, where $S$ denotes a sequence space.	
\end{ex}

\begin{ex} 
	Let $P_n(K)$ be the set of polynomials of degree at most $n$. Let $P(K)$ be the set of all polynomials, and equip it with standard addition of polynomials. Then: 
	\begin{align*}
			P_0(K) \leq P_1(K) \leq P_2(K) \leq \cdots \leq P_n(K) \leq \cdots \leq P 
	\end{align*}	
	The dimension of each $P_n$ is $n+1$. 
\end{ex}

\begin{theorem} 
Let $(V, +, \cdot)$ be a vector space over $K$. 
	\begin{enumerate}
		\item Let $U$ be a subspace of $V$ and let $W$ be a vector space. Then: $U \cap W \leq V$. 
		\item Let $U_1, ..., U_n \leq V$. Then: $U_1 \cap \cdots \cap U_n = \bigcap_{i=1}^n U_i \leq V$. 
		\item Let $I$ be an arbitrary index set, and let $U_i \leq V$ $\forall i \in I$. Then: $\bigcap_{i \in I} V_I \leq V$. 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item Another straightforward verification of the axioms. 
		\begin{enumerate}[noitemsep]
			\item $0 \in U$, and $0 \in W$ since $W$ is a vector space. Thus $0 \in W \cap U$. 
			\item Now we check closure under addition and scalar multiplication.  
			\begin{enumerate}[noitemsep]
				\item Let $v_1, v_2 \in U \cap W$. We thus especially have that $v_1 \in U$ and $v_2 \in U$, and so $v_1 + v_2 \in U$ since $U$ is closed under addition. Similarly, $v_1 + v_2 \in W$. Since the sum is in both sets, it is in the intersection of the sets. Hence, it is closed under addition. 
				\item Since $u_1 \in U$, $K u_1 \in V$ for all $k \in K$, since $U$ is a vector space. Similarly, if $u_2 \in W$, then $k u_2 \in W$ for all $k \in K$. Thus, the product is in the intersection. 
			\end{enumerate}
		\end{enumerate}
		So, by the subspace criteria, $U \cap W \leq V$. 
		\item Exercise. 
		\item Exercise. 
	\end{enumerate}
\end{proof}

So, intersections are generally not a problem. However, unions are a more difficult case. For example let $V = \R^2$, and define $U$ to be the $x$-axis and $W$ to be the $y$-axis. Consider $U \cup W$. It is not a line or plane, so it is not a subspace. While this union of subspaces will always contain zero and while it will always be closed under scalar multiplications, it is \underline{not closed under addition}. Unions in general are not subspaces, which motivates the following theorem. The proof is in assignment two. 

\begin{theorem}
	\vsok. Then, $U \cup V \leq V$ if and only if $U \subseteq W$ or $W \subseteq U$ (i.e., the trivial cases). 
\end{theorem} 
\begin{proof}
	Homework. 
\end{proof}

\begin{definition}[Sum of Vector Spaces]
	\vsok. Let $U, W \leq V$. Then: 
	\begin{align*}
		U + W := \{ w + w\ |\ u \in U,\ w \in W \} 
	\end{align*}
	which is called the \dfn{sum} of $U$ and $W$, is defined as follows: all the possible sums with one vector from $U$ and one vector from $W$. 
\end{definition}

\textbf{\underline{Q}}: Why do we bother defining the sum? 
\begin{enumerate}[noitemsep]
	\item It is a subspace of $V$. 
	\item It is the \emph{smallest} subspace of $V$ that contains the union of $U$ and $W$ (``generator''). 
\end{enumerate}

\begin{theorem}
	\vsok. Let $U, W \leq V$. Then: 
	\begin{enumerate}[noitemsep]
		\item $U + W \leq V$. 
		\item $U+W$ is the \emph{smallest} subspace of $V$ that contains \underline{both} $U$ and $V$ and thus their \underline{union}. This means that if $\widetilde{V}$ is any subspace of $V$ with $U \cup W \subseteq \widetilde{V}$, then the sum $U + W \subseteq \widetilde{V}$. 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item Another straightforward verification of the axioms. 
		\begin{enumerate}[noitemsep]
			\item We have that $0 \in W$ and $0 \in W$, and so $0 = 0 + 0 \in U +W$. 
			\item Let $v_1, v_2 \in U + W$. Then, $\exists$ $u_1, u_2 \in U$, $w_1, w_2 \in W$, such that: 
			\begin{align*}
				& v_1 = u_1 + w_1 \\
				& v_2 = u_2 + w_2 
			\end{align*}
			Adding these together we obtain: 
			\begin{align*}
					v_1 + v_2 = \underbrace{(u_1 + u_2)}_{\in U} + \underbrace{(w_1 + w_2)}_{\in W} \in U + W 
			\end{align*}
			\item Now for scalar multiplication. Write 
			\begin{align*}
				kv_1 & = k(u_1 + w_1) & \\
					& = k u_1 + k w_1 & \text{ (distributivity) } 
			\end{align*}
			$ku_1 \in U$ and $k w_1 \in W$, which means that $kv_1 \in U+W$, which is what we wanted to show. 
			Therefore, $U+W \leq V$.
		\end{enumerate}
		\item Let $\widetilde{V}$ be any subspace of $V$ that contains both $U$ and $W$. Let $u \in U$ and $w \in W$ be arbitrary. THen, by definition, $u+w \in U+W$. Every element bust thus be in $\widetilde{V}$, which implies that $U+W$ must be contained in $\widetilde{V}$, which proves that $U+W$ is the smallest subspace of $V$ containing $U \cup W$. 
	\end{enumerate}
\end{proof}

In a similar fashion we can define $U_1 + ... + U_n$, where $U_1, ..., U_n$ are all subspaces of $V$ as: 
\begin{align*}
		\{ u_1 + ... + u_n\ |\ u_k \in U_k\ \forall\ 1 \leq k \leq n \} 
\end{align*}
Then it's easy to verify that $U_1 + ... + U_n \leq V$. 

\subsection{Linear Combination and Span}

\begin{definition}[Linear Combination]
	\vsok. Let $v_1, ..., v_n \in V$ and let $k_1, ..., k_n \in K$. Then, an expression of the form
	\begin{align}
		k_1 v_1 + ... + k_n v_n 	
	\end{align}	
	is called a \dfn{linear combination} of $v_1, ..., v_n$. 
\end{definition}
Note that it is possible to extend this to infinitely many vectors: 
\begin{align*}
	k_1 v_1 + k_2 v_2 + ...
\end{align*}
but this is \emph{completely meaningless}. This is a bad thing since we do not know what it means to add infinitely many vectors. We have not developed a notion of convergence or distance on a vector space in this class, so we will not discuss this. What we an do is write $k_1 v_1 + k_2 v_2 + ...$ where all but \emph{finitely many} of the $k_i =0$, but this can be viewed as an abuse of notation, since the sum is actually finite. 

\begin{definition}[Span]
	\vsok. Let $v_1, ..., v_n \in V$. Then: 
	\begin{align}
		\vecspan{v_1, ...v_n} := \{ k_1 v_1 + ... + k_n v_n\ |\ 1 \leq i \leq n\ k_i \in K \} 
	\end{align}
	is called the \dfn{span} of $v_1, ..., v_n$. It is the set of all possible linear combinations. We can extend this to infinitely many vectors $v_1, v_2, ... \in V$ as: 
	\begin{align*}
		\vecspan{v_1, ..., v_n } := \{ k_1 v_1 + k_2 v_2 + ...\ |\ k_i \in K, \text{ all but finitely many $k_i$ are zero } \} 	
	\end{align*}
\end{definition}

Note that
\begin{align*}
	\text{span} \{ v_1, ..., v_n \} = \span \{ v_1 \} + ... + \span \{ v_n \} 
\end{align*}
Which means that the span of $v_1, ..., v_n$ can be written as:
\begin{align*}
	\text{span} \{ v_1, ..., v_n \} = \{ k_1 v_1 + ... + k_n v_n\ |\ k_i \in K \} 
\end{align*}
From this, we also obtain that the span of the vectors $v_1, ..., v_n$ is the \emph{smallest} subspace of $V$ containing the vectors $v_1, ..., v_n$. 

\begin{ex}
	Let $V = \R^n$. Then, the standard basis vectors: $e_1 = (1, 0, 0, ...), e_2 = (0,1,0, ...), ..., e_n = (0, 0, ..., 1)$ spans $\R^n$. 	
\end{ex}

\begin{ex}
Consider the vector space $P_n(K)$. The building blocks of this vector space are: 
\begin{align*}
	P_n(K) = \{ 1, x, x^2, x^3, ..., x^n \} 
\end{align*}
We thus have: 
\begin{align*}
	\text{span} \{ 1, x, x^2, ..., x^n \} & = \{ k_0 1 + k_1 x + ... + k_n x^n\ |\ k_i \in K, 0 \leq i \leq n \} \\
		& = P_n(K) 
\end{align*}	
\end{ex}


\begin{ex} 
	Consider the vector space $S$ of all sequences with coefficients in $\R$. Define: 
	\begin{align*}
		s_1 &= (1,0,0,....) \\
		s_2 &= (0,1,0,....) \\
		s_3 & = (0,0,1,0,...) \\
		\vdots 
	\end{align*}	
	This is similar to the previous example where we had the standard basis of $\R^n$. However, $\{ s_1, s_2, ... \}$ does NOT span $s$ since liner combinations only involve finitely many terms. Every linear combination of $s$ results in a sequence that is eventually \emph{constantly equal to zero}, i.e, the subspace $c_{00}$. 
\end{ex}

\subsection{Linear Independence and Dependence}

\begin{definition}[Linear Independence and Dependence]
	\vsok, $v_1, ..., v_n \in V$. We say that $v_1, ..., v_n$ are \dfn{linearly independent} if 
	\begin{align*}
		k_1 v_1 + ... + k_n v_n =0 
	\end{align*}
	implies that $k_1 = k_2 = ... = k_n = 0 $. $v_1, ..., v_n$ are called \dfn{linearly dependent} if they're not linearly independent. 
\end{definition}

\begin{theorem}
	\vsok. Then: 
	\begin{enumerate}[noitemsep]
		\item Any (finite) subset of $V$ that contains the zero vector is linearly dependent. 
		\item If $v_1, ..., v_n$ are linearly dependent, then at least one of them, say $v_j$, can be written as a linear combination of the remaining vectors. In that case, consider span$\{ v_1, ..., v_n\}$. Then we can kick $v_j$ out and still have the same span: 
		\begin{align*}
			\text{span} \{ v_1, ..., v_n \} = \text{span} \{ v_1, ... \hat{v_j}, ..., v_n \} 
		\end{align*}
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}[noitemsep]
		\item Consider $0, v_1, ..., v_n$. This is a linearly dependent set and we can prove it by providing a non-trivial combination that produces zero: 
		\begin{align*}
			1 \cdot 0 + 0 v_1 + ... + 0 v_n = 0 
		\end{align*}
		since the coefficient of the zero vector is non-zero. \textbf{Remark:} even just the set with the zero vector, $\{ 0 \}$, is linearly dependent. This is because $1 \cdot 0 = 0$, which is a non-trivial combination of one vector resulting in the zero vector. 
		\item Let $v_1, ..., v_n$ be linearly dependent. By definition, there exists scalars $k_1, ..., k_n \in K$, not all of them zero, such that: 
		\begin{align*}
			k_1 v_1 + ... + k_j v_j + .... + k_n v_n = 0 
		\end{align*}
		Assume that $k_i \neq 0$. Then, we can solve the equation: 
		\begin{align*}
			k_j v_j = -k_1 v_1 - ... - k_{j-1} v_{j-1} - ... - k_{j+1} v_{j+1} - ... k_n v_n 
		\end{align*}
		Since $k_j \neq 0$, we can divide by $k_j$: 
		\begin{align*}
			v_j = \frac{-k_1}{k_j} v_1 - ... - \frac{-k_n}{k_j} v_n 
		\end{align*}
		Which is a linear combination of the remaining vectors that results in $v_j$. We now want to show that the spans with and without $v_j$ are the same. Obviously, span$\{ v_1, ..., v_{j-1}, v_{j+1}, ..., v_n \}$ is a subset of span$\{ v_1, ..., v_n \}$. We need to prove the opposite inclusion: 
		\begin{align*}
			\text{span} \{ v_1, ..., v_n \} \subseteq \text{span} \{ v_1, ..., \hat{v_j}, ..., v_n \} 
		\end{align*}
	\end{enumerate}
	To that end, take an arbitrary element of the space of $v_1, ..., v_n$: 
	\begin{align*}
		v = a_1 v_1 + ... + a_j v_j + ... + a_n v_n \in \text{span} \{ v_1, ..., v_n \} 
	\end{align*}
	Then, we can write this without $v_j$ by what we did in the above when we solved for $v_j$; 
	\begin{align*}
		v = a_1 v_1 + ... + a_j \left( \frac{-k_1}{k_j} v_1 - ... - \frac{-k_n}{k_j} v_n 	\right) + ... + a_n v_n 
	\end{align*}
	Collecting the coefficients: 
	\begin{align*}
		v = \left( a_1 - \frac{k_1}{k_j} a_j \right) v_1 + ... + \left( a_{j-1}  - \frac{k+{j-1}}{k_j} a_j \right) v_{j-1} + \left(a_{j+1} - \frac{k_{j+1}}{k_j} a_j \right)v_{j+1} + ... + \left(a_n - \frac{k_n}{k_j}a_j \right)v_n 
	\end{align*}
	Which is a linear combination of $v_1, ..., v_{j-1}, v_{j+1}, ..., v_n$. 
\end{proof}

\begin{theorem}
	Let $v_1, ..., v_n \in V$, $v_1, ..., v_n$ be linearly independent. Let $v_{n+1} \notin \text{span} \{ v_1, ..., v_n \}$. Then, $v_1, ..., v_n , v_{n+1}$ are linearly independent. 
\end{theorem}

\begin{proof}
	To show this, we will take an arbitrary linear combination that results in zero, and show that this means that the coefficients must be zero. To that end, let 
	\begin{align*}
		k_1 v_1 + .... + k_n v_n + k_{n+1} v_{n+1} = 0 
	\end{align*}
	be an arbitrary linear combination. We will use the linear independence of $v_1, ..., v_n$ to show that $k_1, ..., k_n = 0$. For $k_{n+1}$, for a contradiction, assume that $k_{n+1} \neq 0$. Then, solve for $v_{n+1}$: 
	\begin{align*}
		k_{n+1} v_{n+1} & = - k_1 v_1 - ... - k_n v_n \\
		\Rightarrow v_{n+1} & = - \frac{k_1}{k_{n+1}} v_1 - \frac{k_n}{k_{n+1}}v_n
	\end{align*}
	Which is impossible, since we assumed that $v_{n+1} \notin$ span$\{ v_1, ..., v_n \}$. This proves that $k_1, ..., k_n , k_{n+1} = 0$, which shows that $v_1, ..., v_n$ must be linearly independent. 
\end{proof}

\begin{definition}[Finite Dimensional] 
	Let $V$ be a vector space over $K$. $V$ is called \dfn{finite-dimensional} if $V$ has a finite spanning set i..e, there are finitely many $v_1, ..., v_n \in V$ such that they all span $V$: 
	\begin{align*}
		V = \text{span} \{ v_1 , ..., v_n \} 
	\end{align*}
\end{definition}

\textbf{Remark:} \vsok. Assume $W \leq V$. Assume that $V$ is finite-dimensional. It is tempting to assume that $W$ is finite-dimensional; this is true, but we do not have any tools to prove it at the moment. This will be proven later. 

\begin{definition}[Infinite-Dimensional]
	$V$ is called \dfn{infinite-dimensional} if it is not finite dimensional. 
\end{definition}
Methods of showing that a vector space is infinite dimensional are very different from methods of showing that a vector space is finite-dimensional; for finite-dimensional, all we need to do is construct one example of a finite spanning set. However, to prove a vector space is infinite dimensional requires showing that no finite spanning set can possibly exist. 

\begin{ex}
	$\R^n$ is finite-dimensional. This is trivial; write out the following finite spanning set: 
	\begin{align*}
		\R^n = \text{span} \{ e_1, ..., e_n \} 
	\end{align*}	
\end{ex}


\begin{ex}
	$c_{00}$ is infinite-dimensional. We will do a proof by contradiction. Assume that $c_{00}$ is finite-dimensional. Let $s_1, ..., s_n$ be sequences in $c_{00}$ such that $c_{00} =$ span$\{ s_1, ..., s_n \}$. 
\end{ex}

No more proofs; just summary. 

\begin{theorem}[Steinitz Exchange Lemma]
	Let $V$ be a finite-dimensional vector space over $K$. Let $u_1, ..., u_m \in V$ be linearly independent. Let $v_1, ..., v_n$ be spanning  (i.e., $V = \text{span} \{ v_1, ..., v_n \}$). Then, $m \leq n$ and there exist finitely many indices $k_1, ..., k_{n-m}$ such that 
	\begin{align}
		u_1, ..., u_m, v_{k},..., v_{k_{n-m}}	
	\end{align}
	is spanning (i.e., we exchange $m$ of the vectors $v_1, ..., v_n$ by $u_1, ..., u_m$ without changing the span.)
\end{theorem}

\subsection{Applications (Theoretical) of Steinitz Exchange Lemma}
\begin{theorem}
	Every subspace of a finite-dimensional vector space is finite-dimensional. 
\end{theorem}

\begin{definition}[Basis]
	Let $V$ be a vector space over $K$. A subset $\mathcal{B}$ of $V$ is called a \dfn{basis} of $V$ if: 
	\begin{enumerate}[noitemsep]
		\item $\mathcal{B}$ is linearly independent. 
		\item $\mathcal{B}$ is spanning. 
	\end{enumerate}
\end{definition}

\begin{ex}
	For $\R^n$, $\mathcal{B} = \{ e_1, ..., e_n \}$. 
\end{ex}


\begin{ex} 
	For $P_n$ (recall these are polynomials of degree less than or equal to $n$), the basis would be $\mathcal{B} = \{ 1, x, ..., x^n \}$. 
\end{ex}

\begin{ex} 
	Set $V = \mat{K}$. Let 
	\begin{align}
		M_{ij}: (m_{ij}) \text{ where } m_{kl} \text{ is } \begin{cases}
			0 & \text{ if } kl  \neq ij \\
			1 & \text{ if } kl = ij
		\end{cases}	
	\end{align}
	Then, $M_{ij}$ is a basis for $V$. 
\end{ex}

\begin{theorem}
	Every finite-dimensional vector space has a basis. 
\end{theorem}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$. Let $\{ u_1, ..., u_m \}$ and $ \{ v_1, ..., v_n \}$ be two bases for $V$. Then $m=n$, i..e, any two bases of a finite-dimensional vector space contain the same number of elements.  
\end{theorem}

\begin{definition}[Dimension]
	Let $V$ be a finite-dimensional vector space over $K$. Let $\{ v_1, ..., v_n \}$ be any basis for $V$. Then, $n$ is the \dfn{dimension} of $V$. 
\end{definition}

\begin{ex}
	\begin{enumerate}[noitemsep]
		\item $\R^n$ as a vector space over $\R$ has dimension $n$. 
		\item $\C^n$ as a vector space over $\C$ has dimension $n$. 
		\item $P_n(K)$ as a vector space over $K$ has dimension $k+1$. 
		\item $\mat{K}$ has dimension $n \times m$. 
	\end{enumerate}	
\end{ex}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$. Let $V = \text{span} \{ B_1, ..., B_n \}$ be a basis for $V$. Let $v \in V$. Then, there exists a uniquely determined $k_1, ..., k_n \in K$ such that
	\begin{align}
		v = k_1 v_1 + ... + k_n v_n 
	\end{align}
\end{theorem}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$. Let $U \leq V$. then, dim$(U) \leq $ dim$(V)$. 
\end{theorem}

\begin{theorem}
	This theorem answers the question: when can we stop constructing bases? Let $V$ be a finite-dimensional vector space over $K$ of dimension $n$. Let $\{ v_1, ..., v_n \}$ be a set of $n$ linearly independent vectors. Then, $\{ v_1, ..., v_n \}$ is a basis for $V$. 
\end{theorem}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$ of dimension $n$. Let $\mathcal{B} = \{ v_1, ..., v_n \}$ be spanning. Then, $\mathcal{B}$ is a basis for $V$. 
\end{theorem}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$. Let $U \leq V$ and let $\mathcal{B}_u := \{ u_1, ..., u_m \}$ be a basis for $U$. Then, $\mathcal{B}_u$ can be extended to a basis for $V$ i.e, $\exists$ $u_{m+1}, ..., u_n \in V$ such that
	\begin{align}
		\mathcal{B}_v := \{ u_1, ..., u_m, u_{m+1}, ..., u_n \} 	
	\end{align}
	is a basis for $V$. 
\end{theorem}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$. Let $U \leq V$ and $W \leq V$. Then: 
	\begin{align}
		\operatorname{dim}(U+W) = \operatorname{dim}(U) + \operatorname{dim}(W) - \operatorname{dim}(U \cap W) 	
	\end{align}
\end{theorem}

\begin{ex}
	Any two planes through the origin in $\R^3$ have non-trivial intersection, i.e., they intersect in at least a line through the origin. One can use the above theorem to give a formal proof of this. 	
\end{ex}

\begin{definition}[Direct Sum]
	\vsok. Let $U \leq V, W \leq V$. Define: 
	\begin{align}
		U + W := \{ u+w\ |\ u \in U, w \in W \} 	
	\end{align}
	Now, the sum is called \dfn{direct} if the following property holds: $\forall$ $v \in U + W$, the representation $v = u + w$, where $u \in U$ and $w \in W$ is uniquely determined. We then write $V = U \oplus W$. 
\end{definition}

\begin{theorem}
	A sum $U+W$ of subspaces of $V$ is direct $\iff$ $U \cap W = \{ 0 \}$. 
\end{theorem}

\section{Linear Maps}
In mathematics one often wants to study maps between objects that preserve structures. In our case, we want to study \dfn{linear maps}.

\begin{definition}[Linear]
	Let $V$ and $W$ be vector spaces over $K$ (it must be the same $K$). A function $L: V \rightarrow W$ is called \dfn{linear} if it behaves well with respect to the operations that we have: 
	\begin{enumerate}[noitemsep]
		\item $L(v_1 + v_2) = L(v_1) + L(v_2)$. 
		\item $L(kv_1) = kL(v_1)$ $\forall k \in K$. 
	\end{enumerate}
\end{definition}  

\begin{theorem}
	Let $L: V \rightarrow W$ be linear. Then the zero vector is preserved: 
	\begin{align}
		L(0) = 0 	
	\end{align}
\end{theorem}
\textbf{Caution!} in calculus the so called ``linear'' maps $f: \R \rightarrow \R$, $x \mapsto ax +b$ is only linear if $b=0$. This is similar to the fact that lines in general are not subspaces. 

\begin{definition}[Subspaces Associated to Linear Maps: Kernel and Image] Let $L: V \rightarrow W$ be linear. Then: 
\begin{enumerate}[noitemsep]
	\item The \dfn{kernel} is defined as: 
	\begin{align}
		\operatorname{ker}(L) := \{ v \in V\ |\ L(v) = 0 \} \subseteq V 	
	\end{align}
	\item The \dfn{image} is defined as: 
	\begin{align}
		\operatorname{im}(L) := \{ L(v)\ |\ v \in V \} \subseteq W	
	\end{align}
\end{enumerate}
\end{definition}

\begin{theorem}
	Let $L: V \rightarrow W$ be linear. Then: 
	\begin{enumerate}[noitemsep]
		\item The kernel is a subspace of $V$. 
		\item The image is a subspace of $W$. 
	\end{enumerate}
\end{theorem}

\begin{ex} 
	Let $A \in \mat{K}$. Then, $\kernel{K}$ is the null space of $A$ and $\image{K}$ is the column space of $A$. 
\end{ex}

\begin{ex} 
	Consider the derivative map: $\mathcal{D}: P(\R) \rightarrow P(\R)$, $p \mapsto p'$. Then $\kernel{\mathcal{D}} = P_0(\R)$ (the set of all constant polynomials) and $\image{\mathcal{D}} = P(\R)$ (everything). 
\end{ex}


\begin{theorem}
	Let $V, W$ be vector spaces over $K$. Let $L: V \rightarrow W$ be linear. Then $L$ is injective $\iff$ $\kernel{K} = \{ 0 \}$. 
\end{theorem}

\begin{theorem}
	This theorem tells us how we can construct linear maps. Let $V$ be a finite-dimensional vector space over $K$, let $W$ be any vector space over $K$. Let $\{ v_1, ..., v_n \}$ be a basis for $V$. Let $\{ w_1, ..., w_n \}$ be arbitrary vectors iN $W$. Then, there exists a uniquely determined linear map $L: V \rightarrow W$ such that
	\begin{align}
		L(v_1) = w_1, ..., L(v_n) = w_n 	
	\end{align}
	This tells us that we can uniquely determined a linear map by its basis. 
\end{theorem}

\begin{theorem}[Dimension Formula -- Fundamental Theorem]
	Let $V$ be a finite-dimensional vector space over $K$, $W$ a vector space over $K$, $L: V \rightarrow W$ a linear map. 
	\begin{align}
		\dim{\kernel{L}} + \dim{ \image{L}} = \dim{V} 
	\end{align}
\end{theorem}

\subsection{Endomorphisms}

\begin{definition}[Endomorphism]
	A linear map $L: V \rightarrow V$ is called a \dfn{endomorphism}. 
\end{definition}

\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$, $L: V \rightarrow V$ an endomorphism. Then 
	\begin{align}
		L \text{ bijective } \iff L \text{ injective } \iff L \text{ surjective} 	
	\end{align}
\end{theorem}

\begin{lemma}
	Let $V$ be a finite-dimensional vector space over $K$ and let $U \leq V$ such that $\dim U \leq \dim V$. Then $U=V$.
\end{lemma}

\subsection{Coordinate Vectors}
\begin{theorem}
	Let $V$ be a finite-dimensional vector space and let $\mathcal{B}$ be an ordered basis for $V$. Then $[ \cdot ]_{\mathcal{B}}: V \rightarrow K^n$, $v \mapsto [v]_{\mathcal{B}}$ is linear and bijective, i.e. such a map is called a \dfn{isomorphism}. 
\end{theorem}


\begin{theorem}
	Let $L: V \rightarrow W$ be an isomorphism. Then, $L$ has an inverse map $L^{-1}: W \rightarrow V$ which is linear. 
\end{theorem}

\begin{theorem}
	Let $V$ be a vector space over $K$ of dimension $n$. Then, $V$ is isomorphic to $K^n$. 
\end{theorem}


\subsection{Representations of Linear Maps as Matrices}
\begin{theorem}
	Let $L: V \rightarrow W$ be linear, $\dim V = m$, $\dim W = n$. Let $\mathcal{B} = \{ v_1, ..., v_m \}$ be a basis for $V$ and let $\mathcal{B'} = \{ w_1, ..., w_n \}$ be a bsis for $W$. Then
	\begin{align}
		[ L(v) ]_{\mathcal{B}'} = [L]_{\mathcal{B'}, \mathcal{B}} [v]_{\mathcal{B}} 	
	\end{align}
	$\forall v \in V$.
\end{theorem}

\begin{theorem}
	Let $L: V \rightarrow W$ be linear, where $V$ and $W$ are finite-dimensional. Let $\mathcal{B}$ be a basis for $V$ and let $\mathcal{B}'$ be a basis for $W$ and define $A := [L]_{\mathcal{B', B}}$. Then: 
	\begin{enumerate}[noitemsep]
		\item $\kernel{L} \simeq \text{nullspace}(A)$.
		\item $\image{L} \simeq \text{colspace}(A)$. 
	\end{enumerate} 
	the isomorphism is given by the following map: 
	\begin{align}
		& \kernel{L} \rightarrow \text{nullspace}(A) \\
		& v \mapsto [v]_\mathcal{B} 	
	\end{align}
	and 
	\begin{align}
		& \image{L} \rightarrow \text{colspace}(A) \\
		& w \mapsto [w]_{\mathcal{B}'} 	
	\end{align}
\end{theorem}

\subsection{Composition of Linear Maps}
\begin{theorem}
	Let $L_1: U_\mathcal{B} \rightarrow V_{\mathcal{B}'} $ and $L_2: V_{\mathcal{B'}} \rightarrow W_{\mathcal{B''}}$ be two linear maps. Let $\dim{U} = m$, $\dim{V} = k$, and $\dim{W} = n$. Then: 
	\begin{align}
		\underbrace{[L_2 \circ L_1]_{\mathcal{B'', B}}}_{n \times m} = \underbrace{[L_2]_{\mathcal{B'', B'}}}_{n \times k} \circ \underbrace{[L_1]_{\mathcal{B', B}}}_{k \times m} 	
	\end{align}
	(Remark: this is why matrix multiplication in Math 133 was defined in such a complicated way). 
\end{theorem}

\subsection{Matrix Representations of Isomorphisms}
\begin{theorem}
	Let $U, V$ be finite-dimensional vector spaces over a field $K$. Let $L: V \rightarrow W$ be linear and let $\mathcal{B}, \mathcal{B'}$ be bases for $V, W$, respectively. Then, $L$ is an isomorphism $\iff$ the matrix representation map $[L]_{\mathcal{B}', \mathcal{B}}$ is invertible. 
\end{theorem}

\subsection{Change of Basis}
Two driving questions of this section: 
\begin{enumerate}[noitemsep]
	\item If $V$ is a finite-dimensional vector space over $K$, $\mathcal{B}, \mathcal{B'}$ are bases for $V$, then how are the coordinate vectors of $v \in V$ related for each of the bases? In other words, let $v \in V$. Given $[v]_{\mathcal{B}}$, how can one compute $[v]_{\mathcal{B'}}$? We call this a \dfn{change of basis}. 
	\item Let $L: V \rightarrow V$ be linear, let $\mathcal{B}, \mathcal{B'}$ be bases for $V$. Given $[L]_{\mathcal{B}, \mathcal{B}}$, how can one compute $[L]_{\mathcal{B'}, \mathcal{B'}}$? 
\end{enumerate}
One can answer 1. question with the \dfn{transition matrix} or \dfn{change of basis matrix} 
\begin{align}
	[v]_{\mathcal{B'}} = [id]_{\mathcal{B', B}} \cdot [v]_{\mathcal{B}} 	
\end{align}
One can answer 2. question similarly.  For an invertible matrix $P$ that goes from $\mathcal{B}$ to $\mathcal{B'}$, we have: 
\begin{align}	
	[L]_{\mathcal{B', B'}} = P^{-1} [L]_{\mathcal{B,B}} P	
\end{align}
This leads us to the notion of similar matrices. 

\begin{definition}[Similar]
	We say that two $n \times n$ matrices are \dfn{similar} there exists an invertible matrix $P \in \matn{K}$ such that
	\begin{align}
		B = P^{-1} A P 	
	\end{align}
\end{definition}

\subsection{The Normal Form Problem}
\begin{definition}[Equivalence Relation] 
	Let $S$ be a set. A \dfn{relation} is a subset of the cartesian product $ S \times S$. An equivalence relation is denoted by $\sim$, and it holds for any relation such that the following properties hold:
	\begin{enumerate}[noitemsep]
		\item Every element in $S$ is in a relation with itself: 
		\begin{align}
			\forall x \in S, x \sim x 	
		\end{align}
		\item (Reflexivity): $\forall x, y \in S$, $x \sim y$ $\iff$ $y \sim x$. 
		\item (Transitivity): $\forall x, y, z \in S$ if $x \sim y \land y \sim z \Rightarrow x \sim z$. 
	\end{enumerate}
	Equivalence relations partitions sets $S$ into pairwise disjoint equivalence classes. 
\end{definition}

\begin{definition}[Equivalence Class]
	Let $S$ be a set. Let $\sim$ be an equivalence relation on $S$. Then, the \dfn{equivalence class} of $S$ is defined as: 
	\begin{align}
		[s]_{\sim} := \{ t \in S\ |\ t \sim s \} 	
	\end{align}
	where $s \in S$. 
\end{definition}

\begin{theorem}
	Let $S$ be a set, $\sim$ an equivalence relation on $S$. The equivalence classes of $\sim$ partition $S$ (i.e, every $s \in S$ is contained in an equivalence class) and any two equivalence classes are either identical or disjoint). 
\end{theorem}

\begin{theorem}
	Similarity is an equivalence relation on the set of all $n \times n$ matrices with coefficients in $K$. That is, the relation
	\begin{align}	
		B \sim A \iff \exists P \in GL(n, K) \text{ such that } B = P^{-1}AP 	
	\end{align}
	is an equivalence relationship. 
\end{theorem}

\textbf{Motivating Question of the Normal Form Problem:} can we within a given similarly class find  one representation or several that is ``simpler'' than the others? These can be more useful, since they can shed light on certain mathematical details. This is the motivation behind eigenvectors and eigenvalues. 

\begin{definition}[Invariant Subspace]
	Let $L: V \rightarrow V$ be a linear map. Let $V$ be a vector space over $K$, can be finite or infinite-dimensional. A subspace $U \leq V$ is called \dfn{L-Invariant} if 
	\begin{align}
		\image{L(u)} \subseteq U 	
	\end{align}
	for example, the kernel of any linear map is invariant since its mapped to zero, and $0 \in U$. 
\end{definition}

\begin{definition}[Eigenvector/Eigenvalue]
	\vsok. Let $L: V \rightarrow V$ be linear. Let $ v \in V$, $v \neq 0$,  such that $L(v) = \lambda v$ for some $\lambda \in K$. Then, $v$ is called a \dfn{eigenvector} of $L$ to the \dfn{eigenvalue} $\lambda$. 
\end{definition}


\begin{theorem}
	Let $V$ be a finite-dimensional vector space over $K$. Let $L: V \rightarrow V$ be a linear map. Let $\lambda$ be an eigenvalue of $L$. Define $E_\lambda := \{ v \in V\ |\ L(v) = \lambda v \}$. Then, $E_\lambda (L) \leq V$. This is called the \dfn{eigenspace} of the eigenvalue $\lambda$. 
\end{theorem}

\begin{theorem}
	$E_\lambda(L)$ is $L$-invariant. 
\end{theorem}

\begin{theorem}
	(Eigenvectors to distinct eigenvalues are linearly dependent). If $L: V \rightarrow V$ is linear, $v_1, ..., v_k$ are eigenvectors of $L$ to eigenvalues $\lambda_1, ..., \lambda_k$ (which are pairwise distinct), then $v_1, ..., v_k$ are linearly independent. 
\end{theorem}

\begin{theorem}[Very Fundamental and Important]
	Let $V$ be a vector space over $K$, $V$ finite-dimensional. Let $L$ be an endomorphism. Then, $L$ is diagonalisable, i.e., there is a basis $\mathcal{B}$ for which $[K]_{\mathcal{B}}$ is a diagonal matrix $\iff$ $V$ has a basis of eigenvectors. 
\end{theorem}

\textbf{Question:} can we find a criteria for a linear map to be diagonalisable? 

\begin{theorem}
	Let $V$ be a vector space over $K$ with dimension of $V$ equal to $n$. Let $L: V \rightarrow V$ be a linear map. Assume that $L$ has a basis of eigenvectors to pairwise distinct eigenvalues. Then, $L$ is diagonalisable. 
\end{theorem}

\textbf{Question:} is every linear map $L: V \rightarrow V$ diagonalisable?
\newline
\newline
\textbf{Answer:} No. Consider the following examples: 
\begin{ex}
	\begin{itemize}
		\item Differentiation map $\mathcal{D}$ is not diagonalisable. 
	\end{itemize}
\end{ex}

\begin{theorem}
	Let $L: V \rightarrow V$. Then, $0$ is an eigenvalue of $L$ $\iff$ $L$ is not injective. In this case, the eigenspace to the eigenvalue $0$ is the kernel. 
\end{theorem}

\subsection{Characteristic Polynomial}
\begin{theorem}
Similar matrices  have the same characteristic polynomial. 	
\end{theorem}

\subsection{Normal Form for Non-Diagonalisable Matrices}
\begin{definition}[Upper/Lower Triangular] 
	Let $A \in \matn{K}$. A matrix of the form: 
	\begin{align}
		A = \begin{bmatrix}
			\lambda_1 & * & * & * & \hdots & * \\
			0 & \lambda_2 & * & * & \hdots & * \\
			0 & 0 & \lambda_3 & * & \hdots & * \\
			0 & 0 & \hdots & \ddots & \hdots & * \\
			0 & 0 & 0 & 0 & 0 & \lambda_n 
		\end{bmatrix}	
	\end{align}
	is called \dfn{upper triangular}. The transpose of such a matrix is called \dfn{lower triangular}. Here, $\lambda_1, ..., \lambda_n$ are the eigenvalues. 
\end{definition}

\begin{theorem}
	Let $A \in \matn{C}$ and let $A$ be upper triangular. Then, the elements on the main diagonal are exactly the eigenvalues of $A$. 
\end{theorem}

\begin{theorem}[``Great Theorem''] 
	Every matrix in $\matn{C}$ is similar to an upper triangular matrix. 
\end{theorem}

\section{Applications of Linear Algebra (to other fields of math...not real life :-P)}
\subsection{Matrix Exponential}
Recall from Math 222 the MacLaurin series of $e$: 
\begin{align}
	e^x & = 1 + x + \frac{x^2}{2!} + ... \\
	& = \sum_{k=0}^\infty \frac{x^k}{k!}	
\end{align}
Can we do the same with matrices? That is, let $X \in \matn{K}$. Does the limit
\begin{align}
	\lim_{N \rightarrow \infty} \sum_{k=0}^N \frac{1}{k!} X^k	
\end{align}
exist in the analytic sense? Yes -- w/o justification, see analysis. The limit is denoted by: 
\begin{align}
	\operatorname{exp}(X) := \sum_{k=0}^\infty \frac{1}{k!} X^k 	
\end{align}
this is called the \dfn{matrix exponential of $X$}. This is used to solve ODEs. In short, the matrix exponential of a diagonal matrix is really nice. One can diagonalise a non-diagonal matrix and take the exponential. If the matrix is not diagonalisable, then one can upper-triangularise it. Then, the matrix exponential gives you a recursive relation which you can then use a computer to solve. 

This can also be used to solve difference equations. 

\section{Orthogonality: A quick introduction}
\subsection{Standard inner product on $\R^n$}
Recall the standard inner product on $\R^n$. Let $u = [u_1, ..., u_n]^t$ and $v = [v_1, ..., v_n]^t$. Then: 
\begin{align}
	\langle u, v \rangle := u_1 v_1 + ... + u_n v_n 
\end{align}
This is called a \dfn{bilinear form}, i.e., it's linear in each component. What does this mean?
\begin{align}
	& \langle u+w , v \rangle = \langle u, v \rangle + \langle w, v \rangle \\
	& \langle ku, v \rangle = k \langle u, v \rangle \\
	& \langle u, v +w \rangle = \langle u, v \rangle + \langle u, w \rangle \\
	& \langle u, kv \rangle = k \langle u, v \rangle	
\end{align}
Now let's go to $\C$. Let $z = x + iy$. Recall the \dfn{complex conjugate} of $z$, $\overline{z}$: 
\begin{align}
	\overline{z} = \overline{x+iy} = x-iy 	
\end{align}
We have the following: 
\begin{align}
	\overline{z} z = | z |^2	
\end{align}
Let's generalise this to $\C^n$. How can we determine the length of a vector in $\C^n$? To do this, first define: 
\begin{align}
	z^* := \overline{z}^t 	
\end{align}
then, 
\begin{align}
	|| z ||^2 = z^* z 	
\end{align}
We will thus define the standard inner product on $\C^n$ as: 
\begin{align}
	\langle z, w \rangle := z^* w = \overline{z}_1 w_1 + ... + \overline{z}_n w_n 	
\end{align}
\subsection{Properties of the Standard Inner Product on $\C^n$}
Caution! The standard inner product on $\C^n$ is not bilinear; it is only \dfn{sesqui-linear}. Here, it is only linear in its FIRST component: 
\begin{align}
	& \langle z, v + w \rangle = \langle z, w \rangle + \langle z, v \rangle \\
	& \langle z, kw \rangle = k \langle z, w \rangle \\
	& \langle z + v, w \rangle = \langle z, w \rangle + \langle v, w \rangle \\
	& \langle kz, w \rangle = \overline{k} \langle z, w \rangle 	
\end{align}

\begin{definition}[Orthogonal]
	Two vectors $z, w \in \C^n$ are called \dfn{orthogonal} if their inner product $\langle z, w \rangle = 0$. 
\end{definition}

\begin{definition}[Unit Vector]
	A vector $v \in K^n$ is a \dfn{unit vector} if $|| v || = \sqrt{\langle v, v \rangle} = 1$. 
\end{definition}

\begin{definition}
	Vectors $\{ v_1, ..., v_n \} \in K^n$ are called \dfn{orthogonal} if $\{ v_1, ..., v_n \}$ are pairwise orthogonal. They're called \dfn{orthonormal} if they're pairwise orthogonal and all the $v_i$ are unit vectors. 
\end{definition}
If $\{ v_1, ..., v_k \}$ are orthonormal and $v = a_1 v_1 + ... = a_n v_n$, then 
\begin{align}
	a_j = \langle v_i, v \rangle 	
\end{align}

\begin{definition}[Orthogonal]
	A matrix $A \in \matn{R}$ is called \dfn{orthogonal} if the columns of $A$ are orthonormal (``for historical reasons -- a bit of an unfortunate thing''). 
\end{definition}

\begin{definition}[Unitary]
	A matrix $A \in \matn{C}$ is called \dfn{unitary} if the columns of $A$ are orthonormal with respect to the inner product on $\C^n$. 
\end{definition}

\subsection{Application of Orthogonality (Orthogonal and Unitary Matrices)}
\subsubsection{Orthogonal Case}
Let $A \in \matn{R}$ be orthogonal. Then, its inverse is $A^t$. 

\subsubsection{Unitary Case}
Let $A \in \matn{C}$ be unitary. Then, its inverse is $A^*$, where $A^* := \overline{A}^t$. 
\newline
\newline 
\textbf{Question:} let $U \leq K^n$. Let $\mathcal{B} = \{ v_1, ..., v_k \}$ be a basis for $U$. How can we find an orthogonal or maybe orthonormal basis $\mathcal{B}'$ for the same subspace? 
\newline
\newline
\textbf{Answer:} Gram-Schmidt process !! 
Let $\mathcal{B} = \{ v_1, ..., v_k \}$ be a basis for $U$. Then, define the following vectors: 
\begin{align*}
	& u_1 = v \\
	& u_2 = v_2 - \frac{\langle u_1, v_2 \rangle}{\langle u_1, u_1 \rangle} u_1 \\
	& u_3 = v_3 - \frac{\langle u_1, v_3 \rangle}{\langle u_1, u_1 \rangle} u_1 - \frac{\langle u_2, v_3 \rangle}{\langle u_2, u_2 \rangle} u_2 \\
	& \vdots \\
	& u_k = v_k - \frac{\langle u_1, v_k \rangle}{\langle u_1, u_1 \rangle} u_1 - ... - \frac{\langle u_{k-1}, u_k \rangle}{\langle u_{k-1}, u_{k-1} \rangle} u_{k-1} 
\end{align*}
When the GS process terminates, make sure you normalise each vector to get an orthonormal basis (as of now, you just have an orthogonal basis). 

\begin{theorem}
	Let $P$ be orthogonal. Then, the row vectors of $P$ are pairwise orthogonal unit vectors.
\end{theorem}

\begin{theorem}
	\begin{enumerate}[noitemsep]
		\item Let $P$, $Q$ be orthogonal. Then, $P^{-1}$ is orthogonal and the product $PQ$ is orthogonal. The set of all orthogonal matrices form a \dfn{subgroup} of general linear with respect to matrix multiplication: 
		\begin{align}
			O(n) := \{ P \in \matn{R}\ |\ P \text{ orthogonal } \} 	
		\end{align}
		\item Let $P, Q$ be unitary. Then $P^{-1}$ is unitary and $PQ$ is unitary. Moreover: 
		\begin{align}
			U(n) := \{ P \in \matn{C}\ |\ P \text{ unitary } \} 	
		\end{align}
		is a group with respect to matrix multiplication, and is in fact a subgroup of $GL(n, \C)$. 
	\end{enumerate}
\end{theorem}

\subsection{Orthogonality and Unitary: Diagonalisation and Upper Triangularisation }

\textbf{Question:} what matrices can be diagonalised or upper triangularised by an orthogonal matrix? What matrix can be diagonalised or upper triangularised by a unitary matrix?

\begin{theorem}[Schur]
	Every matrix $A \in \matn{C}$ can be upper-triangularised by a unitary matrix. 
\end{theorem}

\begin{corollary}
	If $A \in \matn{R}$ has only real eigenvalues, $A$ can be upper-triangularised via an orthogonal matrix. 
\end{corollary}

\begin{theorem}
	Let $A \in $ Symm$(n \times n, \R)$. 
	\begin{enumerate}[noitemsep]
		\item Then, all eigenvalues of $A$ are real. 
		\item Eigenvectors to distinct eigenvalues of $A$ are automatically orthogonal. 
	\end{enumerate}
\end{theorem}

\begin{theorem}
	Let $A \in $ Symm$(n \times n, \R)$.  and let $P \in O(n)$. Then, $B= P^{-1}AP = P^t A P$ is again symmetric. 
\end{theorem}

\begin{theorem}[Spectral Theorem in $\R$]
	A matrix $A \in \matn{R}$ is diagonalisable via an orthogonal matrix $P$ $\iff$ $A$ is symmetric. 
\end{theorem}

\subsubsection{Spectral Theorem: Complex Case}
\textbf{Question:} what types of matrices can be diagonalised via a unitary matrix? 

\begin{definition}[Hermitian] 
	A matrix $A \in \matn{C}$ is called \dfn{hermitian} if $A^* = A$ (complex equivalent of a real symmetric matrix). 
\end{definition}

One may conjecture that $A$ is diagonalisable by a unitary matrix if $A$ is hermitian. However, while it is true that all Hermitian matrices are diagonalisable via a unitary matrix, the converse does not hold. We need some more definitions. 


\begin{definition}[Normal Matrix]
	A matrix $A \in \matn{C}$ is called \dfn{normal} if $AA^* = A^* A$. 
\end{definition}
Many important matrices are normal. 
\begin{theorem}
	The following classes of matrices are normal: 
	\begin{enumerate}[noitemsep]
		\item Real symmetric matrices. 
		\item Skew-symmetric real matrices (all eigenvalues are imaginary). 
		\item Real orthogonal matrices (here, the determinant's absolute value is one; resembles rotations and reflections). 
		\item Hermitian matrices 
		\item Skew-Hermitian matrices ($A^* = -A$). 
		\item Unitary matrices.
	\end{enumerate}
\end{theorem}


\begin{remark}
	If $A$ is normal, $P \in GL(n, \C)$, then $P^{-1}AP$, in general, is not normal. However...
\end{remark}

\begin{theorem}
	Let $A \in \matn{C}$, $P \in U(n)$. Then, $P^{-1}AP = P^*AP$ is normal. 
\end{theorem}

\begin{theorem}
	Let $A \in \matn{C}$ be both normal and upper triangular. Then, $A$ is a diagonal matrix. 
\end{theorem}

\begin{theorem}[Complex Spectral Theorem]
	Let $A \in \matn{C}$. Then, $A$ is diagonalisable by a unitary matrix $\iff$ $A$ is normal. 
\end{theorem}

\begin{definition}
	A real symmetric matrix is called...
	\begin{enumerate}[noitemsep]
		\item \dfn{Positive definite} if all eigenvalues of $A$ are positive. 
		\item \dfn{Positive semi-definite} if all eigenvalues are greater or equal than zero. 
		\item \dfn{Negative definite} if all eigenvalues are strictly less than zero. 
		\item \dfn{Negative semidefinite} if all eigenvalues are less than or equal to zero. 
	\end{enumerate}
\end{definition}

\subsection{Applications of the Real Spectral Theorem}
\subsubsection{Real Quadratic Forms}
\begin{definition}[Real Quadratic Form]
	A \dfn{real quadratic form} is a purely quadratic polynomial in $n$ variables with real coefficients. For example: 
	\begin{align}
		X_1^2 + 4X_1X_2 + X_2^2	
	\end{align}
\end{definition}

\textbf{Question:} How can we determine the range of a quadratic form? 

\textbf{Question:} Can every quadratic form be written as a sum or difference of squares?

\begin{definition}[Positive Definite]
	A quadratic form $X := Q(X_1, ..., X_n)$ is called \dfn{positive definite} if $Q(x) \geq 0$ $\forall X \in \R^n$ and $Q(x) = 0 $ $\iff$ $X =0$. One can equivalently define negative definite. 
\end{definition}

Every quadratic form $Q(X)$ can be written in ``matrix form'', i.e; 
\begin{align}
	Q(X) = X^t A X	
\end{align}

\section{Inner Product Spaces}
There is no notion of length in an abstract vector space. Therefore, the aim is to introduce a notion of lengths and angles to abstract vector spaces, i.e., generalise what we've done with the standard inner product. 

We generalise between $K = \R$ and $K= \C$. 
\subsection{$K = \R$}
\begin{definition}[Inner Product Space] Let $V$ be a vector space over $\R$. A function: $\langle, \rangle: V \times V \rightarrow \R$ is called an \dfn{inner product} on $V$ if: 
\begin{enumerate}[noitemsep]
	\item $\langle, \rangle$ is bilinear
	\item $\langle, \rangle$ is symmetric 
	\item $\langle, \rangle$ is positive definite 
\end{enumerate}
	A vector space with an inner product is called a \dfn{inner product space}. 
\end{definition}

\begin{ex}
	The standard inner product on $\R^n$, $\langle x, y \rangle := x^t y$ is an inner product. 
\end{ex}

\begin{ex}
	Let $V = \R^n$ a positive definite $n \times n$ (which implies symmetric on $\R$ or hermitian on $\C$) matrix $A$ gives us an inner product: 
	\begin{align}
		\langle x, y \rangle := x^t Ay 	
	\end{align}
\end{ex}

\begin{ex} 
Let $V = C[a,b]$: the vector space of all continuous functions on $[a,b]$. A useful inner product in analysis is: 
\begin{align}
	\langle f, g \rangle := \int_a^b f(x) g(x) dx 	
\end{align}
	
\end{ex}

\subsection{Complex Inner Product Spaces}
\begin{definition}[Complex Inner Product Space]
	Let $V$ be a $VS$ over $\C$, $\langle, \rangle : V \times V \rightarrow \C$ such that
	\begin{enumerate}[noitemsep]
		\item $\langle, \rangle$ is sesqui-linear.
		\item Conjugate symmetry: $\forall z , w \in V$: 
		\begin{align}
			\langle z, w \rangle = \overline{\langle w, z \rangle} 	
		\end{align}
	\item Positive definite. 
	\end{enumerate}
	Then, $(V, \langle, \rangle)$ is called a \dfn{complex inner product space}. 
\end{definition}

\begin{ex}
	Let $A \in \matn{C}$ be a Hermitian positive definite matrix. Then $\langle z, w \rangle := z^* A w$ is a complex inner product on $\C^n$. 
\end{ex}

\subsection{Geometry on a Vector Space}
We can define the \dfn{length} on an IPS as: 
\begin{align}
	|| x || := \sqrt{ \langle x, y \rangle } 	
\end{align}
Claim: this satisfies all of the conditions in what mathematics is called a norm. 

\begin{definition}[Norm]
	\vsok. Then, $|| \cdot ||$ (\dfn{norm}) is a function that assigns a length to a vector: $V \rightarrow \R_0^+ = [0, \infty[$ such that
	\begin{enumerate}[noitemsep]
		\item $|| x|| \geq 0$ $\forall x \in V$ and $|| x || = 0 \iff x =0$. 
		\item $|| kx || = |k| || x ||  $ $\forall k \in K, x \in V$. 
		\item \dfn{Triangle Inequality}: $|| x +y || \leq || x || + ||y ||$. 
	\end{enumerate}
\end{definition}

\begin{theorem}
	Let $V$ be an IPS over $K$, $|| x || := \sqrt{ \langle x, x, \rangle }$ $\forall x \in V$. Then, $|| \cdot ||$ is a norm on $V$. 
\end{theorem}

\begin{theorem}[Cauchy-Schwarz Inequality]
	Let $V$ be an IPS over $K$. Then: 
	\begin{align}
		| \langle x, y \rangle | \leq || x|| || y || 	
	\end{align}
	$\forall x, y \in V$. 
\end{theorem}

\begin{lemma}
	$\langle a + \lambda b, a + \lambda b \rangle = \langle a, a \rangle + \lambda \langle a, b \rangle + \overline{\lambda} \overline{ \langle a, b \rangle } + \lambda \overline{\lambda} \langle b, b \rangle $
\end{lemma}


\begin{theorem}[Triangle Inequality]
	Let $x, y \in V$ where $V$ is an IPS. Then: 
	\begin{align}
		|| x + y || \leq || x|| + ||y||	
	\end{align}
	(this is the most important theorem in maths). 
\end{theorem}

\begin{lemma}
	Let $z \in \C$. Then: 
	\begin{enumerate}[noitemsep]
		\item $z + \overline{z} = 2 \operatorname{Re}(z)$
		\item $| \operatorname{Re}(z) | \leq |z|$
	\end{enumerate}
\end{lemma}


\begin{definition}[Angle]
	Let $V$ be a real IPS with an inner product $\langle, \rangle$. Let $x, y \neq 0$ in $V$. Let $\varphi \in [0, \pi]$ be uniquely determined such that
	\begin{align}
		\cos ( \varphi ) = \frac{\langle x, y \rangle}{|| x || || y ||}	 \in [-1, 1]
	\end{align}
	(where the inclusion is thanks to Cauchy Schwarz). Then $\varphi$ is the \dfn{angle} between $x$ and $y$. 
\end{definition}

\begin{definition}
	Let $V$ be a real IPS. We say that $x$ and $y$ are \dfn{orthogonal} if $\langle x, y \rangle = 0$. Note that for $x, y \neq 0$, this is equivalent to the angle between $x$ and $y$ being $\pi /2$.
\end{definition}

\begin{definition}
	Let $V$ be an IPS over $\R$. We say that $x, y \in V$ are orthogonal $\iff$ their inner product is zero. 
\end{definition}

\begin{theorem}[Pythagorean's Theorem] 
	Let $V$ be an IPS. Let $X, Y \in V$ be orthogonal. Then
	\begin{align}
		|| X + Y ||^2 = ||x||^2 + ||Y||^2	
	\end{align}
\end{theorem}

\section{Fourier Series in 30 Minutes [not examinable]}
Consider $V$ the vector space of all continuous functions on $\R$ endowed with the inner product
\begin{align}
	\int_{-\pi}^\pi f(x) g(x) dx	
\end{align}
and consider the following set
\begin{align}
	S := \{ 1, \sin (x) , \cos(x), \sin(2x), \cos(2x), ... \} 	
\end{align}
Claim: the functions in $S$ are pairwise orthogonal: 
\begin{align}
	\int_{- \pi}^\pi \sin (nx) \cos(mx) dx = 0 	
\end{align}
$\forall n \in \N, m \in \N_0$. This follows from the product-to-sum trigonometric identities. Moreover, observe: 
\begin{align}
	& \int_{-\pi}^\pi \sin^2 (nx) dx = \pi\ \forall n \in \N \\
	& \int_{-\pi}^\pi \cos^2 (nx ) dx = \pi\ \forall n \in \N \\
	& \int_{-\pi}^\pi 1 dx = 2 \pi \text{ for } n = 0
\end{align}
Now recall the following: Let's work in $\R^n $ with the standard inner product. Let $v_1, ..., v_n$ be pairwise orthogonal, non-zero vectors. Then, we may write: 
\begin{align}
	v = a_1 v_1 +... + a_n v_n 	
\end{align}
for any vector $v \in \R^n$. Recall that we recovered the coefficients $a_1, ..., a_j$ previously with the formula: 
\begin{align}
	a_j = \frac{\langle v_j, v \rangle}{\langle v_j , v_j \rangle}	
\end{align}
(the proof for that only used linearity in the second argument and positive definiteness). Now let $f(x)$ represent a ``signal'': 
\begin{align}
	f(x ) = c_0 + [ a_1 \cos(x) + ... + a_n \cos (nx) ] + [b_1 \sin(x) + ... + b_n \sin (nx) ] 
\end{align}
We know that $f(x) $ is a linear combination of the members in $S$. Assume that we are given an $f(x)$. \textbf{How can we recover the linear combination of sines and cosines?} Well, we simply apply the formula: 
\begin{align}
	a_n & = \frac{\langle \cos(nx), f(x) \rangle}{\langle \cos(nx), \cos(nx) \rangle}	\\
	& = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos (nx) dx 
\end{align}
This is all linear algebra! No calculus or analysis! Similarly for the $b_n$'s: 
\begin{align}
b_n & = \frac{\langle \sin(nx), f(x) \rangle}{\langle \sin (nx), \sin (nx) \rangle}	\\
& = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin(nx) dx 
\end{align}
and we can recover $c_0$: 
\begin{align}	
	c_0 = \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) dx 	
\end{align}
Drawback: in linear algebra, all sums must be finite. Not all functions can be represented as a finite sum of trigonometric functions; they must also be periodic. 


\textbf{Question:} Can $f(x)$ be represented as an infinite series? Does it converge? 


We will use these formulae as a motivation for the definition of a Fourier series. 

\begin{definition}[Fourier Series]
	Let $f(x)$ be periodic and continuous with period $2 \pi$. Define: 
	\begin{align}
		& a_n := \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos (nx) dx \\
		& b_n := \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin (nx) dx \\
		& c_0 := \frac{1}{2 \pi} \int_{-\pi}^\pi f(x) dx 	
	\end{align}
	Now, the analysis comes; two questions: 
	\begin{enumerate}[noitemsep]
		\item Does it converge?
		\item Will we get $f(x)$ back?
	\end{enumerate}
	If it does converge, we obtain the \dfn{Fourier Series} of $f$, which is entirely generated by orthogonality relations. There is a lot of substantial literature covering these two questions. 
\end{definition}

\begin{theorem}[Dirichlet's Theorem]
	Let $F$ be a periodic function with period $ 2 \pi$ and let it be piecewise continuous and continuously differentiable on $[-\pi, \pi]$. Then, the Fourier series of $f$ converges at all points where $f$ is continuous, and it represents $f$. 
\end{theorem}
This is pretty good news as long as your functions are 
reasonably well-behaved. 

\end{document}